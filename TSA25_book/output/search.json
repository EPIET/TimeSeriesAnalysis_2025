[
  {
    "objectID": "scripts/03-practical.html",
    "href": "scripts/03-practical.html",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "",
    "text": "Expected learning outcomes\nBy the end of the session, participants should be able to:\nYou have been provided with one MS Excel file (tsa_practice.xlsx) containing 2 sheets, one for each of two different diseases (dis1, dis2); and one csv file called (tsa_pumala.csv) about Puumala virus infections in Finland.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "scripts/03-practical.html#learning-3",
    "href": "scripts/03-practical.html#learning-3",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "",
    "text": "Manage surveillance datasets with different date formats\nCreate specific time series objects using tsibble\nPlot surveillance data against time",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis Module",
    "section": "",
    "text": "Welcome page\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Welcome page"
    ]
  },
  {
    "objectID": "scripts/03-practical.html#task-3.1",
    "href": "scripts/03-practical.html#task-3.1",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "Task 3.1",
    "text": "Task 3.1\nAssess visually the reported number of each disease by ISO (epidemiological) week or calendar month for all the data provided (“dis2” is optional).\nWhat diseases do you think they are, judging from their distribution in time?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "scripts/04-practical.html",
    "href": "scripts/04-practical.html",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "",
    "text": "Mortality Surveillance data in Spain\nThe Spanish daily mortality monitoring system is run by the National Centre for Epidemiology in Madrid and gathers data from a stable number of municipalities around the country with a computerised death register; the system is representative of the population of Spain and was developed in 2004 with the objective of identifying exceedances in mortality during the summer period. Data since 2000 were collected retrospectively.\nSessions 4-7 and 11 use data from this mortality surveillance system. Session 10 uses data from the same system, but for only one autonomous community in Spain (Aragón).",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#title-4",
    "href": "scripts/04-practical.html#title-4",
    "title": "3  04-practical",
    "section": "Smoothing and Trends",
    "text": "Smoothing and Trends\n\n# Install pacman if not installed already, and activate it\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nCargando paquete requerido: pacman\n\n# Install, update and activate libraries\npacman::p_load(\n  here, \n  rio, \n  skimr,\n  tsibble,\n  ISOweek,\n  slider,    # for rolling means\n  gtsummary,\n  imputeTS, \n  pander,\n  tidyverse\n)\n\n# Create tsa_theme\ntsa_theme &lt;- theme_bw() + \n        theme(\n            plot.title = element_text(face = \"bold\", \n                                      size = 12),\n            legend.background = element_rect(fill = \"white\", \n                                             size = 4, \n                                             colour = \"white\"),\n            # legend.justification = c(0, 1),\n            legend.position = \"bottom\",\n            panel.grid.minor = element_blank()\n        )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.",
    "crumbs": [
      "Practical exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>04-practical</span>"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#title-4-1",
    "href": "scripts/04-practical.html#title-4-1",
    "title": "3  04-practical",
    "section": "Mortality Surveillance data in Spain",
    "text": "Mortality Surveillance data in Spain\nThe Spanish daily mortality monitoring system is run by the National Centre for Epidemiology in Madrid and gathers data from a stable number of municipalities around the country with a computerised death register; the system is representative of the population of Spain and was developed in 2004 with the objective of identifying exceedances in mortality during the summer period. Data since 2000 were collected retrospectively.\nSessions 4-7 and 11 use data from this mortality surveillance system. Session 10 uses data from the same system, but for only one autonomous community in Spain (Aragón).",
    "crumbs": [
      "Practical exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>04-practical</span>"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#learn-4",
    "href": "scripts/04-practical.html#learn-4",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Expected learning outcomes",
    "text": "Expected learning outcomes\nBy the end of the session, participants should be able to:\n\nDescribe, test and fit a trend in surveillance data (simple smoothing and regression);\nAssess and interpret the significance of trend in surveillance data.\n\nYou are provided with a dataset in R (mortagg.Rdata) which includes variables on week, year, total number of deaths (cases), and population, as well as number of deaths and population among males and females separately.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-1",
    "href": "scripts/04-practical.html#task-4-1",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.1",
    "text": "Task 4.1\nAssess visually how the total registered number of deaths has been behaving in the years with available data. Save any changes to a new dataset named `mortagg.\nDiscuss your results with your peers.\nHow would you proceed in analysing your data to further understand how mortality behaves?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-1-1",
    "href": "scripts/04-practical.html#task-4-1-1",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.1.1 (Optional)",
    "text": "Task 4.1.1 (Optional)\nWould you reach the same conclusions if you were exploring mortality for males and females separately? Discuss with your peers.\n\nMoving average and direct statistical modelling of trends\nMoving averages are simple methods to visualise the general trend of a series after removing some of the random day-to-day variation by smoothing the data. This allows you to browse your data for periodicity and observe the general trend. In other words, smoothing the data may remove “noise” from your time series and can facilitate visual interpretation.\nMoving averages model a time series by calculating the numerical mean of the values adjacent to it. It is calculated for each observation, moving along the time axis: e.g. at each time \\(t\\) and for a window of 5 time units, one way of calculating the moving average is by using the observations at \\(t-2\\), \\(t-1\\), \\(t\\), \\(t+1\\) and \\(t+2\\).\n\n\nRegression\nUsing regression methods against the time variable is a simple and familiar way to look at trends and test the slope with the Wald test provided in the output. The kind of regression and interpretation you will use depends on the nature of your dependent variable – in this case, the number (count) of registered deaths.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-2",
    "href": "scripts/04-practical.html#task-4-2",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.2",
    "text": "Task 4.2\nAssess visually the existence of a long-term trend and periodicity in the `mortagg dataset by using smoothing techniques. Where would you centre the smoothing window and why? What is the effect of using different smoothing window centring options?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-3",
    "href": "scripts/04-practical.html#task-4-3",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.3",
    "text": "Task 4.3\nAssess statistically whether the registered number of deaths has been stable in the years available in your dataset. How well can you model the registered number of deaths by assessing only the long-term trend?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-3-1",
    "href": "scripts/04-practical.html#task-4-3-1",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.3.1 (Optional)",
    "text": "Task 4.3.1 (Optional)\nHow could you take the – potentially changing – population of Spain into account in your analyses? Data on population are provided as variables in mortagg$pop.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-3-2",
    "href": "scripts/04-practical.html#task-4-3-2",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.3.2 (Optional)",
    "text": "Task 4.3.2 (Optional)\nIf you are confident with addressing the tasks above and still have enough time, follow the same procedure with the dis1 dataset. Which regression models may be relevant in this example?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-1",
    "href": "scripts/04-practical.html#solution-4-1",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.1",
    "text": "Help for Task 4.1\nImport the mortality.Rdata dataset.\n\nmortagg &lt;- import(here(\"data\", \"mortagg.csv\"))\n\nInspect the data.\n\nstr(mortagg)\n\n'data.frame':   520 obs. of  6 variables:\n $ year   : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ week   : int  1 2 3 4 5 6 7 8 9 10 ...\n $ cases  : int  6090 6398 6271 6023 5552 5065 4821 4732 4509 4496 ...\n $ cases_m: int  3196 3291 3263 3130 2860 2719 2555 2507 2384 2407 ...\n $ cases_f: int  2894 3107 3008 2893 2692 2346 2266 2225 2125 2089 ...\n $ pop    : int  39953520 39953520 39953520 39953520 39953520 39953520 39953520 39953520 39953520 39953520 ...\n\nsummary(mortagg)\n\n      year           week           cases         cases_m        cases_f    \n Min.   :2000   Min.   : 1.00   Min.   :3748   Min.   :1988   Min.   :1696  \n 1st Qu.:2002   1st Qu.:13.75   1st Qu.:4283   1st Qu.:2273   1st Qu.:1997  \n Median :2004   Median :26.50   Median :4516   Median :2389   Median :2137  \n Mean   :2004   Mean   :26.50   Mean   :4657   Mean   :2458   Mean   :2198  \n 3rd Qu.:2007   3rd Qu.:39.25   3rd Qu.:4915   3rd Qu.:2591   3rd Qu.:2328  \n Max.   :2009   Max.   :52.00   Max.   :7343   Max.   :3721   Max.   :3622  \n      pop          \n Min.   :39953520  \n 1st Qu.:41423520  \n Median :43260892  \n Mean   :43273082  \n 3rd Qu.:45236004  \n Max.   :46367550  \n\nview(mortagg)\n\n\nskimr::skim(mortagg)\n\n\nData summary\n\n\nName\nmortagg\n\n\nNumber of rows\n520\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2004.50\n2.88\n2000\n2002.00\n2004.5\n2007.00\n2009\n▇▇▇▇▇\n\n\nweek\n0\n1\n26.50\n15.02\n1\n13.75\n26.5\n39.25\n52\n▇▇▇▇▇\n\n\ncases\n0\n1\n4656.57\n555.52\n3748\n4283.00\n4516.5\n4915.00\n7343\n▇▇▂▁▁\n\n\ncases_m\n0\n1\n2458.36\n270.83\n1988\n2272.75\n2389.0\n2591.25\n3721\n▇▇▃▁▁\n\n\ncases_f\n0\n1\n2198.22\n291.34\n1696\n1997.00\n2137.0\n2328.00\n3622\n▇▇▂▁▁\n\n\npop\n0\n1\n43273082.00\n2112930.23\n39953520\n41423520.00\n43260892.5\n45236004.00\n46367550\n▅▅▅▂▇\n\n\n\n\n\nCreate a time series object with tsibble, using the total case counts and plot it.\n\nmortz &lt;-\n  mortagg %&gt;%\n  mutate(date_index = make_yearweek(year = year, week = week)) %&gt;%\n  as_tsibble(index = date_index)\n\nggplot(data = mortz) +\n  geom_line(mapping = aes(x = date_index, y = cases)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  labs(x = \"Date\", y = \"Number of Deaths\", title = \"Mortality\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\n## Adjusting y-axis scale\nggplot(data = mortz) +\n  geom_line(mapping = aes(x = date_index, y = cases)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(2000, NA)) +\n  labs(x = \"Date\", y = \"Number of Deaths\", title = \"Mortality\") +\n  tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-1-1",
    "href": "scripts/04-practical.html#solution-4-1-1",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.1.1",
    "text": "Help for Task 4.1.1\n\n## Men and Women\nggplot(data = mortz) +\n  geom_line(mapping = aes(x = date_index, y = cases_m),\n        colour = \"green\",\n        lwd=1.1) +\n  geom_line(mapping = aes(x = date_index, y = cases_f),\n        colour = \"black\",\n        lwd=1.1) + \n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Week\", y = \"Number of Deaths\", title = \"Spain: Number of deaths by sex\") +\n  tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-2",
    "href": "scripts/04-practical.html#solution-4-2",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.2",
    "text": "Help for Task 4.2\nAccording to the slider package description, slider is a package for rolling windows analysis. It means that a given function is repeatedly applied to different “windows” of your data as you step through it. Typical examples of applications of rolling window window functions include moving averages or cumulative sums.\nThe slider family of functions from the slider package contains a collection of functions specifically designed to compute a given operation on a rolling window. An introduction vignette for slider can be found by running the following command: vignette(\"slider\")\nFor each record, create the following various types of moving average.\n\nMA5a: the 5-week moving average, centred on cases.\nMA5b: the 5-week moving average of cases and the 4 previous weeks.\nMA5c: the 5-week moving average of the 5 previous weeks.\n\nCompare results.\n\nmortzma &lt;-\n  mortz %&gt;%\n  mutate(\n    MA5a = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 2,\n      .after = 2,\n      .complete = TRUE\n    ),\n    MA5b = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 4,\n      .complete = TRUE\n    ),\n    MA5c = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = function(x) mean(x[-6], na.rm = TRUE),\n      .before = 5,\n      .complete = TRUE\n    )\n  )\n\n# view first 10 lines of data\nhead(mortzma, 10)\n\n# A tsibble: 10 x 10 [1W]\n    year  week cases cases_m cases_f      pop date_index  MA5a  MA5b  MA5c\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;     &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2000     1  6090    3196    2894 39953520   2000 W01   NA    NA    NA \n 2  2000     2  6398    3291    3107 39953520   2000 W02   NA    NA    NA \n 3  2000     3  6271    3263    3008 39953520   2000 W03 6067.   NA    NA \n 4  2000     4  6023    3130    2893 39953520   2000 W04 5862.   NA    NA \n 5  2000     5  5552    2860    2692 39953520   2000 W05 5546. 6067.   NA \n 6  2000     6  5065    2719    2346 39953520   2000 W06 5239. 5862. 6067.\n 7  2000     7  4821    2555    2266 39953520   2000 W07 4936. 5546. 5862.\n 8  2000     8  4732    2507    2225 39953520   2000 W08 4725. 5239. 5546.\n 9  2000     9  4509    2384    2125 39953520   2000 W09 4571  4936. 5239.\n10  2000    10  4496    2407    2089 39953520   2000 W10 4465  4725. 4936.\n\n\nThe slide_index_dbl is a slider package function designed to run a pre-specified function on a rolling window of a numeric (_dbl) variable, ordered by an index time (date or date-time) variable. A full description of this function can be found by running the following command: ?slide_index_dbl\nThe .x argument provides the vector with the numbers that will be used for computation.\nThe .i argument defines the vector with the time variable that will be used for ordering the data.\nThe .f argument indicates the function that will be used to perform the intended computations. In this case, an arithmetic mean computation is carried out by using the mean function. The na.rm = TRUE is a varying argument included as a ... argument of slide_index_dbl. (Run the expression args(\"slide_index_dbl\") and take notice on the position/sequence of this function’s arguments).\nThe .before argument defines the number of observations before the central time point of a given time window to use in the rolling window computation. In the example above for a 5 time points centred moving average, in MA5a, 2 observations are used before the central point, and 2 observations are used after it, using the .after argument.\nThe complete argument indicates where the computation should be carried in rolling windows that have complete observations.\nWe applied a more complicated function to compute MA5c, taking a backwards moving window of six values but omitting the latest value (current time point) from the calculation of the average.\n\n## wide format dataset ---\nggplot(data = mortzma, mapping = aes(x = date_index)) +\n  geom_line(mapping = aes(y = cases), colour = \"black\") +\n  geom_line(mapping = aes(y = MA5a), colour = \"red\") +\n  geom_line(mapping = aes(y = MA5b), colour = \"blue\") +\n  geom_line(mapping = aes(y = MA5c), colour = \"green\") +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Date\", \n       y = \"Number of Deaths\", \n       title = \"Moving averages\") +\n  tsa_theme\n\n\n\n\n\n\n\n\nWe observe that the calculation is similar across these various methods, but is not aligned to the series in the same way. MA5a is centred in the middle of the period used to calculate the mean. MA5b is placed at the end of the period. MA5c is placed one step forward (smoothing functions can be used for forecasting the following point). The “models” provided are similar for a 5-week window, but the lag is different.\nMoving average is only one way of smoothing. Other ways of smoothing the data to get a general idea of the trend include, for example, LOESS (locally estimated scatterplot smoothing) smoothing, where the contribution of surrounding observations is weighted, i.e. it is not the arithmetical mean for each set (window) of observations.\n\nggplot(mortz, mapping = aes(x = date_index, y = cases)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = TRUE) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Date\", \n       y = \"Number of Deaths\", \n       title = \"Loess smoothing\") +\n  tsa_theme\n\n\n\n\n\n\n\n\nThe stat_smooth provides a smoothing curve using a LOESS method. We can change the degree of smoothing by adding a span option. The span controls the amount of smoothing for the default loess smoother; smaller numbers produce wigglier lines, and larger numbers produce smoother lines.\n\nggplot(data = mortz, mapping = aes(x = date_index, y = cases)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = TRUE, span = 0.1) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Date\", y = \"Number of Deaths\", title = \"Loess smoothing (span = 0.1)\") +\n  tsa_theme\n\n\n\n\n\n\n\n\nTo better observe the general trend, we need to find the length of the moving average that will erase the seasonal component. Various lengths can be tried; here we have used 25, 51 and 103.\n\nmortzma2 &lt;-\n  mortz %&gt;%\n  mutate(\n    MA25 = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 24,\n      .complete = TRUE\n    ),\n    MA51 = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 50,\n      .complete = TRUE\n    ),\n    MA103 = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 102,\n      .complete = TRUE\n    )\n  )\n\n## Visually inspect data\nslice(mortzma2, 20:30)\n\n# A tsibble: 11 x 10 [1W]\n    year  week cases cases_m cases_f      pop date_index  MA25  MA51 MA103\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;     &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2000    20  4123    2207    1916 39953520   2000 W20   NA     NA    NA\n 2  2000    21  3916    2059    1857 39953520   2000 W21   NA     NA    NA\n 3  2000    22  4295    2273    2022 39953520   2000 W22   NA     NA    NA\n 4  2000    23  4050    2168    1882 39953520   2000 W23   NA     NA    NA\n 5  2000    24  4200    2223    1977 39953520   2000 W24   NA     NA    NA\n 6  2000    25  4018    2191    1827 39953520   2000 W25 4708.    NA    NA\n 7  2000    26  4104    2146    1958 39953520   2000 W26 4628.    NA    NA\n 8  2000    27  4041    2155    1886 39953520   2000 W27 4534.    NA    NA\n 9  2000    28  3896    2096    1800 39953520   2000 W28 4439.    NA    NA\n10  2000    29  4021    2142    1879 39953520   2000 W29 4359.    NA    NA\n11  2000    30  3908    2088    1820 39953520   2000 W30 4293.    NA    NA\n\nslice(mortzma2, 45:55)\n\n# A tsibble: 11 x 10 [1W]\n    year  week cases cases_m cases_f      pop date_index  MA25  MA51 MA103\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;     &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2000    45  4399    2383    2016 39953520   2000 W45 4037.   NA     NA\n 2  2000    46  4456    2446    2010 39953520   2000 W46 4058.   NA     NA\n 3  2000    47  4462    2376    2086 39953520   2000 W47 4065.   NA     NA\n 4  2000    48  4444    2326    2118 39953520   2000 W48 4081.   NA     NA\n 5  2000    49  4271    2315    1956 39953520   2000 W49 4084.   NA     NA\n 6  2000    50  4410    2378    2032 39953520   2000 W50 4099.   NA     NA\n 7  2000    51  4521    2414    2107 39953520   2000 W51 4116  4406.    NA\n 8  2000    52  4732    2539    2193 39953520   2000 W52 4144. 4379.    NA\n 9  2001     1  4897    2635    2262 40688520   2001 W01 4184. 4350.    NA\n10  2001     2  4749    2553    2196 40688520   2001 W02 4213. 4320     NA\n11  2001     3  4668    2515    2153 40688520   2001 W03 4243. 4293.    NA\n\nslice(mortzma2, 95:105)\n\n# A tsibble: 11 x 10 [1W]\n    year  week cases cases_m cases_f      pop date_index  MA25  MA51 MA103\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;    &lt;int&gt;     &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2001    43  4263    2296    1967 40688520   2001 W43 4206. 4371.   NA \n 2  2001    44  4223    2265    1958 40688520   2001 W44 4199. 4367.   NA \n 3  2001    45  4286    2313    1973 40688520   2001 W45 4193. 4364.   NA \n 4  2001    46  4717    2483    2234 40688520   2001 W46 4207. 4369.   NA \n 5  2001    47  4642    2496    2146 40688520   2001 W47 4208. 4373.   NA \n 6  2001    48  4816    2546    2270 40688520   2001 W48 4229. 4383.   NA \n 7  2001    49  4723    2502    2221 40688520   2001 W49 4255  4390.   NA \n 8  2001    50  4837    2580    2257 40688520   2001 W50 4279. 4396.   NA \n 9  2001    51  5078    2753    2325 40688520   2001 W51 4303. 4403. 4407.\n10  2001    52  5559    2876    2683 40688520   2001 W52 4353. 4416. 4402.\n11  2002     1  5878    3020    2858 41423520   2002 W01 4423. 4438. 4397.\n\n\nYou can use View to examine the first rows of mortzma2.\n\nView(mortzma2)\n\n\nggplot(mortzma2, mapping = aes(x = date_index)) +\n  geom_line(mapping = aes(y = cases), colour = \"black\", lwd=0.7) +\n  geom_line(mapping = aes(y = MA25), colour = \"red\", lwd=1.1) +\n  geom_line(mapping = aes(y = MA51), colour = \"blue\", lwd=1.1) +\n  geom_line(mapping = aes(y = MA103), colour = \"orange\", lwd=1.1) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Week\", \n       y = \"Number of deaths\", \n       title = \"Moving averages\") +\n  tsa_theme\n\n\n\n\n\n\n\n\nComment on the lines provided by the different smoothing windows. Which one do you think is the best for eliminating seasonality? What would happen if you used an even greater window?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-3",
    "href": "scripts/04-practical.html#solution-4-3",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.3",
    "text": "Help for Task 4.3\nUsing regression against time is a very simple way to look at the trends and test the slope with the Wald test provided.\nWe will use the standard glm function to fit a Poisson regression model.\nFor the regression you will need to create a date variable from the year and week (consecutive number of week).\n\nmortz &lt;-\n  mortz %&gt;%\n  ungroup() %&gt;% \n  mutate(index = seq.int(from = 1, to = nrow(.)))\n\nmort_poissontrend &lt;- glm(\n  formula = cases ~ index,\n  family = \"poisson\",\n  data = mortz\n)\n\nCheck model results.\n\nsummary(mort_poissontrend)\n\n\nCall:\nglm(formula = cases ~ index, family = \"poisson\", data = mortz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 8.407e+00  1.300e-03 6469.40   &lt;2e-16 ***\nindex       1.470e-04  4.282e-06   34.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 32912  on 519  degrees of freedom\nResidual deviance: 31732  on 518  degrees of freedom\nAIC: 37080\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe glm function fits a linear regression model to the log of the number of deaths. You can use the names function to see the various components of the output of the glm function, as above. As mentioned in the previous session, the str function is also often useful for looking at the “structure” of the output of an R function.\nVariables in R models are specified using notation along the lines of response_variable ~ explanatory_variable_1 + explanatory_variable_2 etc. Check the material from the MVA module. A more detailed explanation of how to build model formulas in R can be found in the Details section of the help page of the formula function: ?stats::formula.\nsummary, when given the results of fitting a linear regression model, will provide a summary of the fitted trend and associated statistics.\nIdentify and interpret the intercept and the trend. In a Poisson model on the log(y), the coefficient needs to be exponentiated in order to interpret it for the output y (that is, the original y without the log).\nPlot the fitted values against the observed ones.\n\nggplot(data = mortz, mapping = aes(x = index)) +\n  geom_point(mapping = aes(y = cases), alpha = 0.5) +\n  geom_line(mapping = aes(y = fitted(mort_poissontrend)), colour = \"green\", lwd = 1.1) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Week\", y = \"Number of deaths\", title = \"Number of deaths per week with fitted trend\") +\n  tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-3-1",
    "href": "scripts/04-practical.html#solution-4-3-1",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.3.1",
    "text": "Help for Task 4.3.1\nTo take the population into account, we need to include the population in the model. This is done by adding a term called offset(). The parameter beta for the offset, in this case for the population, will be forced to be 1; in other words, the offset will not affect the outcome. The advantage is that we can interpret the IRR of time in terms of mortality rate = cases/population.\n\nmort_poissontrend_pop &lt;- glm(cases ~ index + offset(log(pop)),\n  data = mortz,\n  family = \"poisson\"\n)\n\nsummary(mort_poissontrend_pop)\n\n\nCall:\nglm(formula = cases ~ index + offset(log(pop)), family = \"poisson\", \n    data = mortz)\n\nCoefficients:\n              Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept) -9.090e+00  1.300e-03 -6990.58   &lt;2e-16 ***\nindex       -1.764e-04  4.285e-06   -41.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 32147  on 519  degrees of freedom\nResidual deviance: 30452  on 518  degrees of freedom\nAIC: 35801\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe summary() function will only provide the coefficients in the original log scale of the poisson model. Using tbl_regression with the argument exponentiate = T we get the IRR coefficient\n\nmort_poissontrend_pop %&gt;% \n  gtsummary::tbl_regression(\n    exponentiate = T,\n    estimate_fun = ~style_number(.x, digits = 4)\n  )\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nindex\n0.9998\n0.9998, 0.9998\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nAnd plot the predicted agains the original values\n\nggplot(data = mortz, mapping = aes(x = index)) +\n  geom_point(mapping = aes(y = cases), alpha = 0.5) +\n  geom_line(mapping = aes(y = fitted(mort_poissontrend_pop)), colour = \"blue\", lwd = 1.1) +\n  #scale_y_continuous(limits = c(0, NA)) +\n  labs(\n    x = \"Time point\", y = \"Number of Deaths\",\n    title = \"Mortality with fitted trend (Poisson)\"\n  ) +\n  tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-3-2",
    "href": "scripts/04-practical.html#solution-4-3-2",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.3.2",
    "text": "Help for Task 4.3.2\nYou can use the salmonellosis example (dis1) to see how an exponential trend might fit the data better.\n\nsource(\"src/tidyverse/session_4.R\")\n\nFor the regression you will still need to create a date variable from the year and week (weeknumber).\n\ndis1 &lt;- import(here(\"data\", \"tsa_practice.xlsx\"), which = \"dis1\")\n\ndis1 &lt;-\n  dis1 %&gt;%\n  mutate(date = seq.int(from = 1, to = nrow(.)))\n\nGenerate a new variable lcases as the natural logarithm of cases:\n\nsalmo &lt;- dis1\n\nsalmo &lt;-\n  salmo %&gt;%\n  mutate(lcases = log(cases))\n\nsalmoz2 &lt;-\n  salmo %&gt;%\n  mutate(date_index = make_yearweek(year = year, week = week)) %&gt;%\n  as_tsibble(index = date_index)\n\nPlot the logarithm of the number of cases according to the time:\n\nggplot(data = salmoz2, mapping = aes(x = date_index)) +\n  geom_line(mapping = aes(y = lcases)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Year\", y = \"Cases (natural log scale)\", title = \"Log Salmonella data\") +\n  tsa_theme\n\n\n\n\n\n\n\n\nFit a model of lcases against date using linear regression.\n\nlogmodel &lt;- lm(lcases ~ date, data = salmoz2)\nsummary(logmodel)\n\n\nCall:\nlm(formula = lcases ~ date, data = salmoz2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.65732 -0.61829  0.08907  0.64319  2.60617 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.0247934  0.0890767   11.51   &lt;2e-16 ***\ndate        0.0099544  0.0003528   28.22   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.89 on 417 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.6563,    Adjusted R-squared:  0.6555 \nF-statistic: 796.2 on 1 and 417 DF,  p-value: &lt; 2.2e-16\n\n\nPlot the log data and the model against time. Note that fitted values cannot be created where the number of cases is missing. Stata seems to do linear interpolation of the fitted values to fill in the gaps, so we will do that too.\n\nfitted_pois_df &lt;-\n  enframe(fitted(logmodel), name = \"date\", value = \"ltrend\") %&gt;%\n  mutate(date = as.integer(date))\n\nsalmoz2 &lt;-\n  salmoz2 %&gt;%\n  left_join(\n    x = .,\n    y = fitted_pois_df,\n    by = \"date\"\n  ) %&gt;%\n  mutate(ltrend = imputeTS::na_interpolation(x = ltrend, option = \"linear\"))\n\nWe use is.na as above to correctly slot the fitted values back into the time series.\nWe then use na.approx, mentioned above, to interpolate missing values.\n\nggplot(data = salmoz2, mapping = aes(x = date)) +\n  geom_line(mapping = aes(y = lcases)) +\n  geom_line(mapping = aes(y = ltrend), colour = \"green\") +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(\n    x = \"Time point\",\n    y = \"Log Salmonella cases\",\n    title = \"Log Salmonella data with fitted trend\"\n  ) +\n  tsa_theme\n\n\n\n\n\n\n\n\nGenerate a new variable (trend), the anti-log of the prediction (ltrend):\n\nsalmoz2 &lt;-\n  salmoz2 %&gt;%\n  mutate(trend = exp(ltrend))\n\nPlot the real data (cases) and this model (trend) according to time:\n\nggplot(data = salmoz2, mapping = aes(x = date)) +\n  geom_line(mapping = aes(y = cases)) +\n  geom_line(mapping = aes(y = trend), colour = \"green\") +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(\n    x = \"Time point\",\n    y = \"Salmonella cases\",\n    title = \"Salmonella data with fitted trend\"\n  ) +\n  tsa_theme\n\n\n\n\n\n\n\n\nCompare the results with those you would have got if you had used linear regression on the original data.\nAlternatively, you can run a Poisson or a negative binomial regression to model the data:\n\nsalmopoismodel &lt;- glm(cases ~ date, data = salmoz2, family = \"poisson\")\nsummary(salmopoismodel)\n\n\nCall:\nglm(formula = cases ~ date, family = \"poisson\", data = salmoz2)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 1.310e+00  2.483e-02   52.76   &lt;2e-16 ***\ndate        1.000e-02  7.118e-05  140.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 40792  on 418  degrees of freedom\nResidual deviance: 12557  on 417  degrees of freedom\n  (12 observations deleted due to missingness)\nAIC: 14688\n\nNumber of Fisher Scoring iterations: 5\n\n\nThere is more on using Poisson regression later on.\n\nfitted_pois2_df &lt;-\n  enframe(fitted(salmopoismodel), name = \"date\", value = \"trend2\") %&gt;%\n  mutate(date = as.integer(date))\n\nsalmoz2 &lt;-\n  salmoz2 %&gt;%\n  left_join(\n    x = .,\n    y = fitted_pois2_df,\n    by = \"date\"\n  ) %&gt;%\n  mutate(trend2 = imputeTS::na_interpolation(x = trend2, option = \"linear\"))\n\n\nggplot(data = salmoz2, mapping = aes(x = date)) +\n  geom_line(mapping = aes(y = cases)) +\n  geom_line(mapping = aes(y = trend2), colour = \"green\") +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(\n    x = \"Time point\",\n    y = \"Salmonella cases\",\n    title = \"Salmonella data with fitted trend\"\n  ) +\n  tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/03-practical.html#solution-3-1",
    "href": "scripts/03-practical.html#solution-3-1",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "Help for Task 3.1",
    "text": "Help for Task 3.1\nRequired source code.\n\n# Install pacman if not installed already, and activate it\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n# Install, update and activate libraries\npacman::p_load(\n  here, \n  rio, \n  skimr,\n  tsibble,\n  ISOweek,\n  tidyverse\n)\n\nImport your data to R from these Excel files and examine it using the well-known str, summary and skim functions\n\ndis1 &lt;- import(here(\"data\", \"tsa_practice.xlsx\"), which = \"dis1\")\n\n\nstr(dis1)\n\n'data.frame':   431 obs. of  3 variables:\n $ year : num  1981 1981 1981 1981 1981 ...\n $ week : num  1 2 3 4 5 6 7 8 9 10 ...\n $ cases: num  1 3 NA 4 3 2 3 4 1 NA ...\n\n\n\nsummary(dis1)\n\n      year           week           cases       \n Min.   :1981   Min.   : 1.00   Min.   :  1.00  \n 1st Qu.:1983   1st Qu.:12.50   1st Qu.:  9.00  \n Median :1985   Median :26.00   Median : 28.00  \n Mean   :1985   Mean   :25.86   Mean   : 65.16  \n 3rd Qu.:1987   3rd Qu.:39.00   3rd Qu.: 83.50  \n Max.   :1989   Max.   :52.00   Max.   :554.00  \n                                NA's   :12      \n\n\n\nskim(dis1) %&gt;% as_tibble()\n\n# A tibble: 3 × 12\n  skim_type skim_variable n_missing complete_rate numeric.mean numeric.sd\n  &lt;chr&gt;     &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 numeric   year                  0         1           1985.        2.40\n2 numeric   week                  0         1             25.9      15.2 \n3 numeric   cases                12         0.972         65.2      96.3 \n# ℹ 6 more variables: numeric.p0 &lt;dbl&gt;, numeric.p25 &lt;dbl&gt;, numeric.p50 &lt;dbl&gt;,\n#   numeric.p75 &lt;dbl&gt;, numeric.p100 &lt;dbl&gt;, numeric.hist &lt;chr&gt;\n\n\nSince we will work with time series data, we could use some time to explore the most relevant variable: date (in this case, the year variable). dis1 is a dataset containing weekly counts of a certain disease between 1981 and 1989. A typical year has 52 weeks, thus we can count the number of times each year appear in the data using table, tabyl or count function, your to-go tools for categorical variables\n\ntable(dis1$year)\n\n\n1981 1982 1983 1984 1985 1986 1987 1988 1989 \n  52   52   52   52   52   52   52   52   15 \n\n\n\njanitor::tabyl(dis1$year)\n\n dis1$year  n    percent\n      1981 52 0.12064965\n      1982 52 0.12064965\n      1983 52 0.12064965\n      1984 52 0.12064965\n      1985 52 0.12064965\n      1986 52 0.12064965\n      1987 52 0.12064965\n      1988 52 0.12064965\n      1989 15 0.03480278\n\n\nEach one provides different outputs and possesses unique capabilities. In general, tabyl should be your preferred basic function, thanks to its tidy integration and format output. It also enables some customization and data manipulation.\n\nR has a collection of packages for handling time series data.\nSome of these packages are part of the tidyverts family of packages. Particularly, it includes the tsibble package, which intends to create a data infrastructure for easier manipulation and handling of temporal data, and adapts the principles of the).\nAccording to the help description, a tsibble object is defined by\n\nan index, as a variable with inherent ordering from past to present.\na key, as a set of variables that define observational units over time.\nuniquely identified observations by an index and a key.\n\nTo convert the original dis1 dataset to a tsibble object, we use the as_tsibble function and specify the index, i.e. the variable specifying the time unit of interest. In our case, this is year and week variables. Variables that are not used for index or key purposes, such as the cases variables, are considered as measured variables.\n\ndis1z &lt;-\n    dis1 %&gt;%\n    mutate(date_index = make_yearweek(year = year, week = week)) %&gt;%\n    as_tsibble(index = date_index)\n\nstr(dis1z)\n\ntbl_ts [431 × 4] (S3: tbl_ts/tbl_df/tbl/data.frame)\n $ year      : num [1:431] 1981 1981 1981 1981 1981 ...\n $ week      : num [1:431] 1 2 3 4 5 6 7 8 9 10 ...\n $ cases     : num [1:431] 1 3 NA 4 3 2 3 4 1 NA ...\n $ date_index: week [1:431] 1981 W01, 1981 W02, 1981 W03, 1981 W04, 1981 W05, 1981 W0...\n   ..@ week_start: num 1\n - attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ .rows: list&lt;int&gt; [1:1] \n  .. ..$ : int [1:431] 1 2 3 4 5 6 7 8 9 10 ...\n  .. ..@ ptype: int(0) \n - attr(*, \"index\")= chr \"date_index\"\n  ..- attr(*, \"ordered\")= logi TRUE\n - attr(*, \"index2\")= chr \"date_index\"\n - attr(*, \"interval\")= interval [1:1] 1W\n  ..@ .regular: logi TRUE\n\nhead(dis1z, 10)\n\n# A tsibble: 10 x 4 [1W]\n    year  week cases date_index\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;week&gt;\n 1  1981     1     1   1981 W01\n 2  1981     2     3   1981 W02\n 3  1981     3    NA   1981 W03\n 4  1981     4     4   1981 W04\n 5  1981     5     3   1981 W05\n 6  1981     6     2   1981 W06\n 7  1981     7     3   1981 W07\n 8  1981     8     4   1981 W08\n 9  1981     9     1   1981 W09\n10  1981    10    NA   1981 W10\n\n\nYou can see how the number of disease 1 cases is distributed in time using the ggplot package.\n\nggplot(data = dis1z) +\n    geom_line(mapping = aes(x = date_index, y = cases)) +\n    scale_x_yearweek(date_labels = \"%Y\") +\n    labs(x = \"Date\", y = \"Number of Cases\", title = \"Disease 1 data\") +\n    theme_bw() + \n        theme(\n            plot.title = element_text(face = \"bold\", \n                                      size = 12),\n            legend.background = element_rect(fill = \"white\", \n                                             size = 4, \n                                             colour = \"white\"),\n            # legend.justification = c(0, 1),\n            legend.position = \"bottom\",\n            panel.grid.minor = element_blank()\n        )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n# We can save theme modifications into a single object and then use it in new plots\ntsa_theme &lt;- theme_bw() + \n        theme(\n            plot.title = element_text(face = \"bold\", \n                                      size = 12),\n            legend.background = element_rect(fill = \"white\", \n                                             size = 4, \n                                             colour = \"white\"),\n            # legend.justification = c(0, 1),\n            legend.position = \"bottom\",\n            panel.grid.minor = element_blank()\n        )\n\nNote from the plot that there are missing values in the data.\nIf data points are collected at regular time interval, we can correct missingness by providing a default a default value or interpolating missing values from other data. The tsibble package has the fill_gaps function, which fills missing values with a pre-specified default value. It is quite common to replaces NAs with its previous observation for each origin in time series analysis, which is easily done using fill function from tidyr package.\nA quick overview of implicit missing values with tsibble is available on vignette(\"implicit-na\").\nAs the focus of this training is on understanding principles of time series analysis rather than visualisation of time series, we will mainly use the ggplot functions for the remaining exercises.\nOur colleagues shared some data with us they produced with Stata. They forgot to save it as a excell or csv. Luckily for us, the function import is here to save the day\n\ndis3 &lt;- import(here(\"data\", \"tsa_pumala.csv\"))\n\nHere you have one variable for the year, one variable for the month and one variable with the complete date in days, but in a text (chr class) format.\n\nstr(dis3)\n\n'data.frame':   3844 obs. of  3 variables:\n $ year    : int  1995 1995 1995 1995 1995 1995 1995 1995 1995 1995 ...\n $ month   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ date_str: chr  \"02/01/1995\" \"02/01/1995\" \"03/01/1995\" \"03/01/1995\" ...\n\nhead(dis3)\n\n  year month   date_str\n1 1995     1 02/01/1995\n2 1995     1 02/01/1995\n3 1995     1 03/01/1995\n4 1995     1 03/01/1995\n5 1995     1 04/01/1995\n6 1995     1 04/01/1995\n\nsummary(dis3)\n\n      year          month          date_str        \n Min.   :1995   Min.   : 1.000   Length:3844       \n 1st Qu.:1998   1st Qu.: 6.000   Class :character  \n Median :2000   Median : 9.000   Mode  :character  \n Mean   :2000   Mean   : 7.907                     \n 3rd Qu.:2002   3rd Qu.:11.000                     \n Max.   :2004   Max.   :12.000                     \n\n\nWith the complete date, you can generate ISO weeks, but first you need the date variable to be converted from text to something R recognises as a date. The lubridate package has a number of convenient functions for converting text strings to dates.\n\nstr(dis3$date_str)\n\n chr [1:3844] \"02/01/1995\" \"02/01/1995\" \"03/01/1995\" \"03/01/1995\" ...\n\ndis3 &lt;- dis3 %&gt;%\n    mutate(my_date = dmy(date_str))\n\nstr(dis3$my_date)\n\n Date[1:3844], format: \"1995-01-02\" \"1995-01-02\" \"1995-01-03\" \"1995-01-03\" \"1995-01-04\" ...\n\n\nAs a complementary note, check the help for strftime to learn about different date formats in R (?strftime).\nTo convert to ISO week, we can use the ISOweek function from the ISOweek package, which creates a new variable representing the ISO week. We could then also create a new variable representing the first Monday of each ISO week. Although the popular lubridate package has an isoweek function (there is also a similar function in the surveillance package), we use the ISOweek package here as it has the ISOweek2date function. If epidemiological weeks are required, use the EpiWeek package.\n\ndis3 &lt;- dis3 %&gt;%\n    mutate(\n        date_isowk = ISOweek(my_date),\n        isodate = ISOweek2date(paste(date_isowk, \"-1\", sep = \"\"))\n    )\n\nhead(dis3)\n\n  year month   date_str    my_date date_isowk    isodate\n1 1995     1 02/01/1995 1995-01-02   1995-W01 1995-01-02\n2 1995     1 02/01/1995 1995-01-02   1995-W01 1995-01-02\n3 1995     1 03/01/1995 1995-01-03   1995-W01 1995-01-02\n4 1995     1 03/01/1995 1995-01-03   1995-W01 1995-01-02\n5 1995     1 04/01/1995 1995-01-04   1995-W01 1995-01-02\n6 1995     1 04/01/1995 1995-01-04   1995-W01 1995-01-02\n\n\nThe paste command concatenates text.\nHere we are adding \"-01\" onto the end of the ISO week variable (which is formatted something like \"1995-W01\"), to indicate that we want the first day of that week, and then supplying that to the ISOweek2date function, which converts that to a date.\nHave a look at the new variables that have been created. date_isowk is the ISO week variable, in string format, and isodate is the Monday of each ISO week. Note that the years 1998 and 2004 each have an ISO week 53.\nYou have several observations in the same week since you have data from different days and one value corresponds to one case. Use the count function to aggregate the data.\n\ndis3_v2 &lt;- dis3 %&gt;%\n    count(date_isowk)\n\nhead(dis3_v2)\n\n  date_isowk  n\n1   1995-W01 13\n2   1995-W02 15\n3   1995-W03  6\n4   1995-W04  5\n5   1995-W05 11\n6   1995-W06 10\n\n\n\ndis3z &lt;- dis3_v2 %&gt;%\n    mutate(date_index = yearweek(x = date_isowk, week_start = 1L)) %&gt;%\n    as_tsibble(index = date_index)\n\nstr(dis3z)\n\ntbl_ts [491 × 3] (S3: tbl_ts/tbl_df/tbl/data.frame)\n $ date_isowk: chr [1:491] \"1995-W01\" \"1995-W02\" \"1995-W03\" \"1995-W04\" ...\n $ n         : int [1:491] 13 15 6 5 11 10 7 6 3 1 ...\n $ date_index: week [1:491] 1995 W01, 1995 W02, 1995 W03, 1995 W04, 1995 W05, 1995 W0...\n   ..@ week_start: int 1\n - attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ .rows: list&lt;int&gt; [1:1] \n  .. ..$ : int [1:491] 1 2 3 4 5 6 7 8 9 10 ...\n  .. ..@ ptype: int(0) \n - attr(*, \"index\")= chr \"date_index\"\n  ..- attr(*, \"ordered\")= logi TRUE\n - attr(*, \"index2\")= chr \"date_index\"\n - attr(*, \"interval\")= interval [1:1] 1W\n  ..@ .regular: logi TRUE\n\nhead(dis3z, 10)\n\n# A tsibble: 10 x 3 [1W]\n   date_isowk     n date_index\n   &lt;chr&gt;      &lt;int&gt;     &lt;week&gt;\n 1 1995-W01      13   1995 W01\n 2 1995-W02      15   1995 W02\n 3 1995-W03       6   1995 W03\n 4 1995-W04       5   1995 W04\n 5 1995-W05      11   1995 W05\n 6 1995-W06      10   1995 W06\n 7 1995-W07       7   1995 W07\n 8 1995-W08       6   1995 W08\n 9 1995-W09       3   1995 W09\n10 1995-W10       1   1995 W10\n\nggplot(data = dis3z) +\n    geom_line(mapping = aes(x = date_index, y = n)) +\n    scale_x_yearweek(date_labels = \"%Y\") +\n    labs(x = \"Date\", y = \"Number of Cases\", title = \"Disease 1 data\") +\n    tsa_theme\n\n\n\n\n\n\n\n\nAggregating cases by month is another possibility.\n\ndis3_agg &lt;- dis3 %&gt;%\n    count(year, month)\n\ndis3z_v2 &lt;- dis3_agg %&gt;%\n    mutate(date_index = make_yearmonth(year = year, month = month)) %&gt;%\n    as_tsibble(index = date_index)\n\nggplot(data = dis3z_v2) +\n    geom_line(mapping = aes(x = date_index, y = n)) +\n    scale_x_yearweek(date_labels = \"%Y\") +\n    labs(x = \"Date\", y = \"Number of Cases\", title = \"Disease 1 data\") +\n    tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "scripts/03-practical.html#solution-3-1-1",
    "href": "scripts/03-practical.html#solution-3-1-1",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "Help for Optional Task 3.1.1",
    "text": "Help for Optional Task 3.1.1\nImport your data to R from these Excel files.\n\ndis2 &lt;- import(here(\"data\", \"tsa_practice.xlsx\"), which = \"dis2\")\n\nInspect the data.\n\nstr(dis2)\n\n'data.frame':   44 obs. of  13 variables:\n $ year: num  1928 1929 1930 1931 1932 ...\n $ Jan : num  609 288 316 750 146 ...\n $ Feb : num  1516 241 781 2010 270 ...\n $ Mar : num  4952 347 1870 4858 621 ...\n $ Apr : num  7466 378 5309 6172 1096 ...\n $ May : num  11155 498 7499 7095 2271 ...\n $ Jun : num  7002 324 5386 4238 2537 ...\n $ Jul : num  1315 151 1386 907 1081 ...\n $ Aug : num  189 69 211 165 288 139 95 358 213 247 ...\n $ Sep : num  74 44 107 43 118 50 47 67 87 84 ...\n $ Oct : num  119 42 128 54 149 55 45 136 99 90 ...\n $ Nov : num  287 34 219 93 594 71 86 333 154 136 ...\n $ Dec : num  324 121 423 134 1183 ...\n\nhead(dis2)\n\n  year  Jan  Feb  Mar  Apr   May  Jun  Jul Aug Sep Oct Nov  Dec\n1 1928  609 1516 4952 7466 11155 7002 1315 189  74 119 287  324\n2 1929  288  241  347  378   498  324  151  69  44  42  34  121\n3 1930  316  781 1870 5309  7499 5386 1386 211 107 128 219  423\n4 1931  750 2010 4858 6172  7095 4238  907 165  43  54  93  134\n5 1932  146  270  621 1096  2271 2537 1081 288 118 149 594 1183\n6 1933 1960 4699 9635 9573  6605 2697  601 139  50  55  71  131\n\nsummary(dis2)\n\n      year           Jan              Feb               Mar         \n Min.   :1928   Min.   :  39.0   Min.   :   52.0   Min.   :   57.0  \n 1st Qu.:1939   1st Qu.: 195.2   1st Qu.:  256.5   1st Qu.:  416.8  \n Median :1950   Median : 302.0   Median :  692.0   Median : 1190.0  \n Mean   :1950   Mean   : 903.8   Mean   : 1750.2   Mean   : 3357.4  \n 3rd Qu.:1960   3rd Qu.:1478.2   3rd Qu.: 2083.2   3rd Qu.: 5188.8  \n Max.   :1971   Max.   :6336.0   Max.   :13226.0   Max.   :25826.0  \n      Apr               May               Jun              Jul        \n Min.   :   78.0   Min.   :   83.0   Min.   :  79.0   Min.   :  35.0  \n 1st Qu.:  622.2   1st Qu.:  700.8   1st Qu.: 724.2   1st Qu.: 340.0  \n Median : 1577.0   Median : 2110.0   Median :1809.5   Median : 598.5  \n Mean   : 3891.2   Mean   : 3379.6   Mean   :2246.2   Mean   : 713.5  \n 3rd Qu.: 6605.2   3rd Qu.: 5809.0   3rd Qu.:3153.5   3rd Qu.:1027.8  \n Max.   :22741.0   Max.   :11155.0   Max.   :7002.0   Max.   :1975.0  \n      Aug              Sep              Oct             Nov        \n Min.   : 28.00   Min.   : 18.00   Min.   : 11.0   Min.   :  12.0  \n 1st Qu.: 98.75   1st Qu.: 43.75   1st Qu.: 53.5   1st Qu.:  56.5  \n Median :187.00   Median : 70.50   Median : 82.5   Median : 112.5  \n Mean   :189.75   Mean   : 79.73   Mean   :100.5   Mean   : 193.0  \n 3rd Qu.:245.50   3rd Qu.:107.50   3rd Qu.:124.2   3rd Qu.: 236.0  \n Max.   :453.00   Max.   :184.00   Max.   :354.0   Max.   :1050.0  \n      Dec        \n Min.   :  21.0  \n 1st Qu.: 101.8  \n Median : 160.0  \n Mean   : 447.9  \n 3rd Qu.: 572.5  \n Max.   :2996.0  \n\n\n\nView(dis2)\n\nYou have separate columns containing the counts. To plot this data, you need to first reshape your dataset by converting it from the current wide format to a long format.\nThe function pivot_longer can perform such transformation.\n\ndis2l &lt;- dis2 %&gt;%\n  pivot_longer(\n      cols = -year, \n      names_to = \"month\", \n      values_to = \"case\"\n  ) %&gt;%\n  mutate(month = as_factor(month)) %&gt;%  # as_factor sets levels in the order they appear\n  arrange(year, month)\n\nstr(dis2l)\n\ntibble [528 × 3] (S3: tbl_df/tbl/data.frame)\n $ year : num [1:528] 1928 1928 1928 1928 1928 ...\n $ month: Factor w/ 12 levels \"Jan\",\"Feb\",\"Mar\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ case : num [1:528] 609 1516 4952 7466 11155 ...\n\nhead(dis2l)\n\n# A tibble: 6 × 3\n   year month  case\n  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n1  1928 Jan     609\n2  1928 Feb    1516\n3  1928 Mar    4952\n4  1928 Apr    7466\n5  1928 May   11155\n6  1928 Jun    7002\n\ndis2l_agg &lt;- dis2l %&gt;%\n    mutate(date_index = make_yearmonth(year = year, month = month)) %&gt;%\n    as_tsibble(index = date_index)\n\nhead(dis2l_agg)\n\n# A tsibble: 6 x 4 [1M]\n   year month  case date_index\n  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;      &lt;mth&gt;\n1  1928 Jan     609  1928 ene.\n2  1928 Feb    1516  1928 feb.\n3  1928 Mar    4952  1928 mar.\n4  1928 Apr    7466  1928 abr.\n5  1928 May   11155  1928 may.\n6  1928 Jun    7002  1928 jun.\n\nggplot(data = dis2l_agg) +\n    geom_line(mapping = aes(x = date_index, y = case)) +\n    scale_x_yearweek(date_labels = \"%Y\") +\n    labs(x = \"Date\", y = \"Number of Cases\", title = \"Disease 3 data\") +\n    tsa_theme\n\n\n\n\n\n\n\n\ndis1 corresponds to salmonellosis cases, dis2 to measles cases in New York.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "scripts/05-practical.html",
    "href": "scripts/05-practical.html",
    "title": "Practical Session 5: Periodicity",
    "section": "",
    "text": "Periodicity\n# Install pacman if not installed already, and activate it\nrm(list=ls())\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n# Install, update and activate libraries\npacman::p_load(\n  here, \n  rio, \n  skimr,\n  tsibble,\n  TSA,\n  tidyverse,\n  season,\n  lmtest\n)\n\n# Create tsa_theme from previous exercise to be used in ggplot.\ntsa_theme &lt;- theme_bw() + \n        theme(\n            plot.title = element_text(face = \"bold\", \n                                      size = 12),\n            legend.background = element_rect(fill = \"white\", \n                                             size = 4, \n                                             colour = \"white\"),\n            # legend.justification = c(0, 1),\n            legend.position = \"bottom\",\n            panel.grid.minor = element_blank()\n        )",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#learn-5",
    "href": "scripts/05-practical.html#learn-5",
    "title": "Practical Session 5: Periodicity",
    "section": "Expected learning outcomes",
    "text": "Expected learning outcomes\nBy the end of this session, participants should be able to:\n\nassess the existence of periodicity in surveillance data\nfit and interpret models containing a trend and one or several sine and cosine curves on surveillance data to model both trend and periodicity",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#task-5-1",
    "href": "scripts/05-practical.html#task-5-1",
    "title": "Practical Session 5: Periodicity",
    "section": "Task 5.1",
    "text": "Task 5.1\nUsing mortagg.Rdata, visually assess the existence of any periodicity in the total number of deaths (cyclical patterns). What do you think the period is, if any? Discuss your results with your peers.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#task-5-2",
    "href": "scripts/05-practical.html#task-5-2",
    "title": "Practical Session 5: Periodicity",
    "section": "Task 5.2",
    "text": "Task 5.2\nUsing mortagg, check statistically for the existence of trend and periodicity. Then, fit a model on the data with a trend and then the relevant period(s).\nHow many different periods are you including in your model? Test statistically if the inclusion of more than one period contributes to the fit of the model. Discuss your results.\nDuring the year, when is mortality highest? When is it lowest? What do you think might be the reason and how would you test for that statistically?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#task-5-4",
    "href": "scripts/05-practical.html#task-5-4",
    "title": "Practical Session 5: Periodicity",
    "section": "Task 5.4 (Optional)",
    "text": "Task 5.4 (Optional)\nAssess the existence of periodicity in the dis1 dataset.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#solution-5-1",
    "href": "scripts/05-practical.html#solution-5-1",
    "title": "Practical Session 5: Periodicity",
    "section": "Help for Task 5.1",
    "text": "Help for Task 5.1\nRequired source code.\n\n# Read data\nload(here(\"data\", \"mortagg2_case_4.RData\"))\n#\n## \n#mortz &lt;-\n#    mortagg %&gt;%\n#    mutate(date_index = make_yearweek(year = year, week = week)) %&gt;%\n#    as_tsibble(index = date_index)\n#\n## Create an index that goes from 1 to the number of rows in the dataset. This \n## variable with represent time. \n#\n#mortz$index &lt;- seq.int(from = 1,\n#                      to = nrow(mortz))\n\nStep 1)\nExplore the time series. The function decompose() decomposes the time series by applying loess to the series; loess is a smoothing method (https://en.wikipedia.org/wiki/Local_regression).\n\n# format the data to a time series object using ts().\nmortz.ts  &lt;- ts(mortz$cases, frequency=52, start=c(2010,1))\n\nplot(decompose(mortz.ts))\n\n\n\n\n\n\n\n\nDiscuss the panels obtained. What features seem to be relevant?\nStep 2)\nThis step has already been done in case study 4. Run the code in order to have at hand the results and be able to compare later with other models.\nWe start by inspecting if there is a linear trend in the data.\n\nmort_trend &lt;- glm(cases ~ index, family = \"poisson\", data = mortz)\n\nsummary(mort_trend)\n\n\nCall:\nglm(formula = cases ~ index, family = \"poisson\", data = mortz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 8.910e+00  1.007e-03 8850.25   &lt;2e-16 ***\nindex       1.895e-04  3.302e-06   57.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 59215  on 519  degrees of freedom\nAIC: 64841\n\nNumber of Fisher Scoring iterations: 4\n\n# incidence rate ratio per week:\nIRR_week &lt;- exp(coef(mort_trend))\nIRR_week\n\n(Intercept)       index \n7408.911392    1.000189 \n\n\n\nggplot(data = mortz, aes(x = index)) +\n    geom_point(aes(y = cases), alpha = 0.4) +\n    geom_line(\n        mapping = aes(y = fitted(mort_trend)),\n        colour = \"green\",\n        lwd=1.5\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n    labs(x = \"Index\", \n         y = \"Number of deaths\", \n         title = \"Regression model: trend\") +\n    tsa_theme\n\n\n\n\n\n\n\n\nInterpret the weekly incidence rate ratio (IRR).\nStep 2)\nAs there seem to be some periodicity in the first exploration, we proceed to inspect if there is a seasonality component in the time series. Seasonality, periodicity both refer to reoccurring patterns in the data.\nR has a number of functions which produce a periodogram - we will use the periodogram function from the TSA package.\n\nmort_period &lt;- TSA::periodogram(mortz$cases)\n\n\n\n\n\n\n\n\nThe periodogram function plots the estimated periodogram for a given time series and also returns various useful outputs as a list which we can use for other analyses.\nThis type of plot is interpreted by identifying peaks. Convert the frequencies at which peaks occur to periods by taking the reciprocal.\n\nmort_period_recip &lt;-\n    tibble(\n        freq = mort_period$freq,\n        spec = mort_period$spec\n    ) %&gt;%\n    arrange(desc(spec)) %&gt;% \n    mutate(reciprocal_freq = 1 / freq)  # here in weeks\n\nview(mort_period_recip)\n\nThe above lines of code extract two variables (freq and spec) from the mort_period_recip object and place them in a tibble data.frame. Both columns are ordered in descending order of the spectral variable. At the top of the table we find the strongest spectral signal, as well as at what periodicity does it appear (see reciprocal frequency). Observe that the reciprocal frequency is in the same units as the data, in this case is in\nweeks.\n\n# breaks every 13, equivalent to every 3 months approximately.\n\nggplot(data = mort_period_recip, aes(x = reciprocal_freq, y = 2*spec)) +\n    geom_line() +\n    scale_x_continuous(limits = c(0, 160), breaks=seq(0, 160, 13)) +\n    labs(x = \"Period (weeks)\", \n         y = \"Spectral density\") +\n    tsa_theme\n\n\n\n\n\n\n\n\nWhat is the main periodicity in the mortality data? (use the plot and the table of mort_period_recip).\nIs there another periodicity that is worth checking out?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#solution-5-2",
    "href": "scripts/05-practical.html#solution-5-2",
    "title": "Practical Session 5: Periodicity",
    "section": "Help for Task 5.2",
    "text": "Help for Task 5.2\nAs the periodogram shows periodicity close to 52 weeks, we will use a sine curve of a 52-week period. Note that periodicity with a period of one year is also referred to as seasonality.\nFit a poisson regression of cases with only sine and cosine predictor term. In order to have an appropriate phase in your model (you do not have to worry about identifying it; this will happen automatically), you need to use both a sine and a cosine curve with the same period. The sum of these two curves gives the periodicity for the specified period and the phase best describing our data.\nThe function cosinor fits a model with sinus and cosinus. You need to specify the unit of time as well as the number of cycles per year. Here the data is weekly, and we assume that there is one cycle per year since we assume a 52-week period.\n\n# cosinor() only works with data.frames, not tibble.\nmortz.df &lt;- as.data.frame(mortz)\n\nmort_sincos &lt;-  cosinor(cases ~ 1, date = \"week\", \n                            data = mortz.df, type = \"weekly\", cycles=1,\n                            family = poisson())\n\nsummary(mort_sincos)\n\n              Length Class  Mode     \ncall           13    -none- call     \nglm            30    glm    list     \nfitted.plus   521    -none- numeric  \nfitted.values 521    -none- numeric  \nresiduals     521    -none- numeric  \ndate            1    -none- character\n\nsummary(mort_sincos$glm)\n\n\nCall:\nglm(formula = f, family = family, data = data, offset = offset)\n\nCoefficients:\n             Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept) 8.9557059  0.0004987 17956.87   &lt;2e-16 ***\ncosw        0.1208466  0.0007047   171.50   &lt;2e-16 ***\nsinw        0.0668199  0.0007027    95.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 23972  on 518  degrees of freedom\nAIC: 29600\n\nNumber of Fisher Scoring iterations: 3\n\n# Sinus and cosinus curves\nggplot(data=mort_sincos$glm$data, aes(x=week, y=sinw)) +\n    geom_line() +\n    geom_line(aes(x=week, y=cosw)) +\n    scale_x_continuous(limits = c(1, 53)) +\n    labs(x = \"Period (weeks)\", \n         y = \"sincos\") +\n    tsa_theme\n\n\n\n\n\n\n\nggplot(data = mortz, aes(x = index)) +\n    geom_point(aes(y = cases), alpha = 0.4) +\n    geom_line(\n        mapping = aes(y = fitted(mort_sincos)),\n        colour = \"green\",\n        lwd=1.5\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n    labs(x = \"Index\", \n         y = \"Number of deaths\", \n         title = \"Regression model: sin, cos terms\") +\n    tsa_theme\n\n\n\n\n\n\n\n# Plot residuals\nplot(mort_sincos$res)\n\n\n\n\n\n\n\n\nWhen you inspect the residuals of the model with only sinus and cosinus terms, what do you observe?\nIn the next model include a trend and the seasonality. We will calculate the sinus and cosinus by hand.\n\n# calculate the sine and cosine terms.\n\nmortz &lt;- mortz %&gt;%\n  mutate(cos52 = cos(2 * pi * index / 52),\n         sin52 = sin(2 * pi * index / 52))\n\nmort_trendsincos &lt;- glm(cases ~ index + sin52 + cos52, family = \"poisson\", data = mortz)\n\nsummary(mort_trendsincos)\n\n\nCall:\nglm(formula = cases ~ index + sin52 + cos52, family = \"poisson\", \n    data = mortz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 8.898e+00  1.012e-03 8792.82   &lt;2e-16 ***\nindex       2.184e-04  3.311e-06   65.97   &lt;2e-16 ***\nsin52       9.137e-02  7.066e-04  129.29   &lt;2e-16 ***\ncos52       1.061e-01  7.033e-04  150.88   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 19668  on 517  degrees of freedom\nAIC: 25298\n\nNumber of Fisher Scoring iterations: 3\n\nggplot(data = mortz, aes(x = index)) +\n    geom_point(aes(y = cases), alpha = 0.4) +\n    geom_line(\n        mapping = aes(y = fitted(mort_trendsincos)),\n        colour = \"green\",\n        lwd=1.5\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n    labs(x = \"Index\", \n         y = \"Number of deaths\", \n         title = \"Regression model: trend, sin, cos terms\") +\n    tsa_theme\n\n\n\n\n\n\n\n# Plot residuals\nplot(mort_trendsincos$residual)\n\n\n\n\n\n\n\n\nHow does the fit look visually? Add a line with the fitted values of the model with only sine and cosine terms. Which models seems to fit better?\nWhat do you observe in the plot of residuals?\nTo fit the model better, we could try to add more sine/cosine curves, of a period corresponding to the second strongest peak in the model (26 weeks). This will allow for cycles (periods) of not only 52, but also 26 weeks, should we think there might be half-yearly cycles which are relevant. In this case, for instance, there may be elevated mortality in winter, but also in summer during heatwaves.\nGenerate sine and cosine terms with a period of 26 weeks and name them sin26 and cos26. Add these two new terms to the previous model.\nPlot the results, compare with the previous model and comment on it.\n\nmortz &lt;-\n    mortz %&gt;%\n    mutate(\n        sin26 = sin(2 * pi * index / 26),\n        cos26 = cos(2 * pi * index / 26)\n    )\n\nmort_trendsin2cos2 &lt;- glm(cases ~ index + sin52 + cos52 + sin26 + cos26,\n                           family = \"poisson\", \n                           data = mortz)\n                           \nsummary(mort_trendsin2cos2)\n\n\nCall:\nglm(formula = cases ~ index + sin52 + cos52 + sin26 + cos26, \n    family = \"poisson\", data = mortz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 8.895e+00  1.013e-03 8779.51   &lt;2e-16 ***\nindex       2.265e-04  3.313e-06   68.35   &lt;2e-16 ***\nsin52       9.034e-02  7.142e-04  126.48   &lt;2e-16 ***\ncos52       1.021e-01  6.994e-04  145.94   &lt;2e-16 ***\nsin26       5.075e-02  7.054e-04   71.94   &lt;2e-16 ***\ncos26       3.298e-02  7.034e-04   46.88   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 12288  on 515  degrees of freedom\nAIC: 17922\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nggplot(data = mortz, aes(x = index)) +\n    geom_point(aes(y = cases), alpha = 0.4) +\n    geom_line(\n        mapping = aes(y = fitted(mort_trendsin2cos2)),\n        colour = \"green\",\n        lwd=1.5\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n    labs(x = \"Index\", \n         y = \"Number of deaths\", \n         title = \"Regression model: trend, sin52, cos52, sin26, cos26 terms\") +\n    tsa_theme\n\n\n\n\n\n\n\n# Plot residuals\nplot(mort_trendsin2cos2$res)\n\n\n\n\n\n\n\n\nIs the addition of variables sin26 and cos26 a statistically significant contribution to your model?\n\n# likelihood ratio test: compares two nested models\n\n# test sin26 and cos26\nlrtest(mort_trendsincos, mort_trendsin2cos2)\n\nLikelihood ratio test\n\nModel 1: cases ~ index + sin52 + cos52\nModel 2: cases ~ index + sin52 + cos52 + sin26 + cos26\n  #Df   LogLik Df Chisq Pr(&gt;Chisq)    \n1   4 -12644.9                        \n2   6  -8954.9  2  7380  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio test compares two nested models, that is, models with the same variables and data, where one model has additional variables. Here,\nthe only difference between mort_trendsincos and mort_trendsin2cos2 are the sine and cosine at 26 weeks. If the test results in a significant p-value, then the additional variables improve the model significantly, and there is a better fit with the additional variables.\n\nsave(list = ls(pattern = 'mort'), file = here(\"data\", \"mortagg2_case_5.RData\"))\n\n#load(here(\"data\",\"mortagg2_case_5.RData\"))",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/07-practical.html",
    "href": "scripts/07-practical.html",
    "title": "Practical Session 7: Forecasting",
    "section": "",
    "text": "Expected learning outcomes\nBy the end of this session, participants should be able to: - understand the use of forecasting in public health surveillance data - forecast the expected number of cases of a disease into the future",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#task-7-1",
    "href": "scripts/07-practical.html#task-7-1",
    "title": "Practical Session 7: Forecasting",
    "section": "Task 7.1",
    "text": "Task 7.1\nJanuary 2020: Your boss received a phone call from the Ministry of Health. She is part of a committee that is responsible for setting the alert levels for mortality in Spain for the next year (2020). Before doing so, she gives you the task to forecast the total expected number of deaths for 2020 based on the historical data up to and including 2019.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#task-7-2",
    "href": "scripts/07-practical.html#task-7-2",
    "title": "Practical Session 7: Forecasting",
    "section": "Task 7.2 (Optional)",
    "text": "Task 7.2 (Optional)\nJanuary 2021: A committee member is interested to get a better understanding of when there were periods of unusually high excess mortality and asks you to provide an analysis that highlights the time when these occurred.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#task-7-3",
    "href": "scripts/07-practical.html#task-7-3",
    "title": "Practical Session 7: Forecasting",
    "section": "Task 7.3 (Optional)",
    "text": "Task 7.3 (Optional)\nJanuary 2021: your boss needs to inform the Ministry of Health about excess deaths so far during the first year of the pandemic.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#solution-7-1",
    "href": "scripts/07-practical.html#solution-7-1",
    "title": "Practical Session 7: Forecasting",
    "section": "Help for Task 7.1",
    "text": "Help for Task 7.1\n\nggplot(data = mortz) +\n    geom_line(\n        mapping = aes(x = year_week, y = cases),\n        colour = \"black\",\n        alpha = 1.2\n    ) +\n    geom_line(\n        mapping = aes(x = year_week, y = mort_sine2cos2trendmodel$fitted),\n        colour = \"red\",\n        alpha = 1.2\n    ) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"8 weeks\") +\n    labs(x = \"Year Week\", y = \"Weekly cases\") +\n    tsa_theme + \n    theme(axis.text.x = element_text(angle = 30, hjust = 1)) \n\n\n\n\n\n\n\n\nThe dataset has records up to 2019-W52, and you would like to project the data (forecast) into 2020.\nFirst, create a new time series object for 2020. Note that 2020 has 53 weeks.\n\npred.df &lt;-\n    tibble(\n        year=2020,\n        week  = 1:53,\n        index = 521:(521+53-1), # add 53 weeks\n        year_week = make_yearweek(year = year, week = week),\n        sin52 = sin(2 * pi * week / 52),\n        cos52 = cos(2 * pi * week / 52),\n        sin26 = sin(2 * pi * week / 26),\n        cos26 = cos(2 * pi * week / 26),\n        pop = 47318050) %&gt;%                # Spanish pop in 2020 (1st Jan)\n    as_tsibble(index = index)\n\nview(pred.df)\n\nCalculate the expected values and 95% prediction intervals for each week of 2020 assuming that the Poisson regression model with trend and 2 seasonality terms based on the previous years is appropriate.\nPredict and plot the expected number of deaths per week for 2020 with prediction intervals:\n\n# apply mort_sine2cos2trendmodel to new data and predict cases with C.I. using bootstrapping.\n# bootstrapping is a statistical method where you draw random samples from your data, \n# and analyze each of this samples.\n# https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n\n\nset.seed(12589)\npred.mort &lt;- ciTools::add_ci(pred.df,\n                            mort_sine2cos2trendmodel,\n                            names=c(\"lPI\", \"uPI\"),\n                            yhatName=\"pred_cases\",\n                            response=TRUE,\n                            type=\"boot\",\n                            nSims=1000)\nhead(pred.mort, 5)\n\n  year week index year_week     sin52     cos52     sin26     cos26      pop\n1 2020    1   521  2020 W01 0.1205367 0.9927089 0.2393157 0.9709418 47318050\n2 2020    2   522  2020 W02 0.2393157 0.9709418 0.4647232 0.8854560 47318050\n3 2020    3   523  2020 W03 0.3546049 0.9350162 0.6631227 0.7485107 47318050\n4 2020    4   524  2020 W04 0.4647232 0.8854560 0.8229839 0.5680647 47318050\n5 2020    5   525  2020 W05 0.5680647 0.8229839 0.9350162 0.3546049 47318050\n  pred_cases      lPI       uPI\n1   9668.633 9505.031  9847.676\n2   9816.630 9649.887 10004.593\n3   9917.123 9743.460 10116.956\n4   9965.433 9788.115 10167.868\n5   9959.873 9784.202 10162.163\n\n\n\nggplot(data = pred.mort) +\n    geom_line(\n        mapping = aes(x = year_week, y = pred_cases),\n        colour = \"red\",\n        alpha = 0.7\n    ) +\n    geom_ribbon(\n        mapping = aes(x = year_week, ymin = lPI, ymax = uPI),\n        fill = \"red\",\n        alpha = 0.1\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"4 weeks\") +\n    labs(x = \"Year Week\", y = \"Predicted weekly fatalities\") +\n    tsa_theme + \n    theme(axis.text.x = element_text(angle = 30, hjust = 1)) \n\n\n\n\n\n\n\n\nCalculate the total number of expected deaths in 2020 in Spain.\n\n# Total predicted cases in 2020, with lPI and uPI\npred.mort %&gt;% \n  summarise(\n    pred_cases_2020 = sum(pred_cases),\n    pred_cases_2020_lPI = sum(lPI),\n    pred_cases_2020_uPI = sum(uPI)\n  )\n\n  pred_cases_2020 pred_cases_2020_lPI pred_cases_2020_uPI\n1        441385.8            435448.6            447514.4",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#solution-7-2",
    "href": "scripts/07-practical.html#solution-7-2",
    "title": "Practical Session 7: Forecasting",
    "section": "Help for Task 7.2",
    "text": "Help for Task 7.2\nCUSUM (cumulative sum) is a graphical method that can be used to determine when there is a change in a process (all-cause mortality in this example). In TSA it can also be used to decide whether there is a need to revise the model e.g. include a covariate or there have been changes in the seasonality.\nUsing expected values from the previous regression model, you can calculate the cumulative sum of the differences between the weekly observed and expected numbers of deaths:\n\nmortz &lt;- mortz %&gt;%\n  mutate(fit_cases = mort_sine2cos2trendmodel$fit,  # get predicted cases\n         \n         # calculate differences\n         difference = cases - fit_cases,\n         cumsum_excess = cumsum(difference),\n         diff_zero = cases - mean(fit_cases),\n         cumsum_zero = cumsum(diff_zero))\n\nPlot this cumulative sum of the residuals.\n\nggplot(data = mortz) +\ngeom_line(\n        mapping = aes(x = year_week, y = diff_zero),\n        colour = \"green\",\n        alpha = 0.7,\n        lwd = 2\n    ) +\n    geom_line(\n        mapping = aes(x = year_week, y = cumsum_zero),\n        colour = \"orange\",\n        alpha = 0.7,\n        lwd = 2\n    ) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n    labs(x = \"Year\", y = \"Cumulative excess cases\") +\n    tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#solution-7-3",
    "href": "scripts/07-practical.html#solution-7-3",
    "title": "Practical Session 7: Forecasting",
    "section": "Help for Task 7.3",
    "text": "Help for Task 7.3\nPlot the actual number of deaths and compare with the predictions from the model based on 2010-2019.\n\n# load mortality data for 2020\nmort2020 &lt;- import(here(\"data\", \"mortagg2020.csv\"))\n\n# order mort2020 from week 1 to week 53, same as in pred.mort\nmort2020 &lt;- mort2020 %&gt;% arrange(week)\n\n# add actual deaths from 2020 in pred.mort\npred.mort &lt;- pred.mort %&gt;% \n  mutate(cases = mort2020$cases)\n\nggplot(data = pred.mort) +\n    geom_line(\n        mapping = aes(x = year_week, y = pred_cases),\n        colour = \"red\",\n        alpha = 0.7,\n        lwd = 1.2\n    ) +\n    geom_ribbon(\n        mapping = aes(x = year_week, ymin = lPI, ymax = uPI),\n        fill = \"red\",\n        alpha = 0.1\n    ) +\n    geom_line(\n        mapping = aes(x = year_week, y = cases),\n        colour = \"black\",\n        alpha = 0.7,\n        lwd = 1.2\n    ) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"4 weeks\") +\n    labs(x = \"Year Week\", y = \"Predicted weekly cases\") +\n    tsa_theme + \n    theme(axis.text.x = element_text(angle = 30, hjust = 1)) \n\n\n\n\n\n\n\n\nExcess deaths can be calculated as the difference of the actual number of fatalities per week and the predicted mean, or the predicted upper 95% PI.\n\npred.mort &lt;- pred.mort %&gt;%\n  mutate(\n    difference_mean = cases - pred_cases,\n    cusum_excess_mean = cumsum(difference_mean),\n    difference_uPI = cases - uPI,\n    cusum_excess_uPI = cumsum(difference_uPI)\n  )\n\n\nggplot(data = pred.mort) +\n    geom_line(\n        mapping = aes(x = year_week, y = cusum_excess_mean),\n        colour = \"orange\",\n        alpha = 0.7,\n        lwd = 2\n    ) +\n    geom_line(\n        mapping = aes(x = year_week, y = cusum_excess_uPI),\n        colour = \"blue\",\n        alpha = 0.7,\n        lwd = 2\n    ) +   \n    #scale_y_continuous(limits = c(-100, NA)) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"4 weeks\") +\n    labs(x = \"Year\", y = \"Cumulative excess cases\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\n# save(list = ls(pattern = 'mort'), file = here(\"data\", \"mortagg2_case_7.RData\"))\n#load(here(\"data\",\"mortagg2_case_7.RData\"))",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  }
]