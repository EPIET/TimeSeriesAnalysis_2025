[
  {
    "objectID": "scripts/06-practical.html",
    "href": "scripts/06-practical.html",
    "title": "Practical Session 6: Residuals and autocorrelation",
    "section": "",
    "text": "Residuals patterns in Surveillance data",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 6: Residuals and autocorrelation"
    ]
  },
  {
    "objectID": "scripts/06-practical.html#learn-6",
    "href": "scripts/06-practical.html#learn-6",
    "title": "Practical Session 6: Residuals and autocorrelation",
    "section": "Expected learning outcomes",
    "text": "Expected learning outcomes\nBy the end of this session, participants should be able to:\n\ndecide on remaining actions when modelling surveillance data after you have accounted for trend and periodicity\ndetect autocorrelation in the residuals of models on surveillance data",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 6: Residuals and autocorrelation"
    ]
  },
  {
    "objectID": "scripts/06-practical.html#task-6-1",
    "href": "scripts/06-practical.html#task-6-1",
    "title": "Practical Session 6: Residuals and autocorrelation",
    "section": "Task 6.1",
    "text": "Task 6.1\nStart by fitting the model with trend and two cosines and two sines. Assuming you are happy with that Poisson regression model from the previous session, check whether this regression model was appropriate in this case.\n\n\nShow the code\nload(here(\"data\",\"mortagg2_case_5.RData\"))\n\n\nStart by fitting the model with trend and two cosines and two sines.\nThen, assuming you are happy with that model, check whether this regression model was appropriate in this case.\nPlot the residuals from the model in case study 5 that included a trend, 52-week periodicity and 26-week periodicity.\n\n\nShow the code\nmortz &lt;-\n    mortz %&gt;%\n    mutate(res=residuals(mort_trendsin2cos2)\n    )\n\n\n\n\nShow the code\nggplot(data = mortz) +\n    geom_point(\n        mapping = aes(x = date_index, y = res)\n    ) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n    labs(x = \"Year week\", \n         y = \"Residuals\", \n         title = \"Spain: number of deaths. Residuals model trend + 2 sine + 2 cosine.\"\n    ) +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nStart by examining the residuals against time. Do you see any additional patterns?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 6: Residuals and autocorrelation"
    ]
  },
  {
    "objectID": "scripts/06-practical.html#task-6-2",
    "href": "scripts/06-practical.html#task-6-2",
    "title": "Practical Session 6: Residuals and autocorrelation",
    "section": "Task 6.2",
    "text": "Task 6.2\nIs there any other periodicity that can be observed in the residuals?\nTo assess whether there is any more periodicity in the data, one can produce a periodogram of the residuals (use code from case study 5).\n\n\nShow the code\nmort_residual_period &lt;- TSA::periodogram(mortz$res)\n\n\n\n\n\n\n\n\n\nShow the code\nmort_residual_period_recip &lt;-\n    tibble(\n        freq = mort_residual_period$freq,\n        spec = mort_residual_period$spec\n    ) %&gt;%\n    arrange(desc(spec)) %&gt;% \n    mutate(reciprocal_freq = 1 / freq)  # here in weeks\n\nview(mort_residual_period_recip)\n\n# breaks every 13, equivalent to every 3 months approximately.\nggplot(data = mort_residual_period_recip, aes(x = reciprocal_freq, y = 2*spec)) +\n    geom_line() +\n    scale_x_continuous(limits = c(0, 160), breaks=seq(0, 160, 13)) +\n    labs(x = \"Period (weeks)\", \n         y = \"Spectral density\",\n         title=\"Periodogram residuals of model with trend + 2 sine + 2 cosine.\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nPlot the residuals again, but now using lines.\n\n\nShow the code\nmax.index &lt;- max(mortz$index)\n\nggplot(data = mortz) +\n    geom_line(\n        mapping = aes(x = index, y = res)\n    ) +\n    labs(\n        x = \"Week\",\n        y = \"Residuals\",\n        title = \"Regression model with trend + 2 sine + 2 cosine.\"\n    ) +\n    scale_x_continuous(limits = c(0, max.index), breaks=seq(0, max.index, 77)) +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nBased on these latest plots, do you see a patterns that can explain the periodogram? Does the periodicity makes sense in this case?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 6: Residuals and autocorrelation"
    ]
  },
  {
    "objectID": "scripts/06-practical.html#task-6-3",
    "href": "scripts/06-practical.html#task-6-3",
    "title": "Practical Session 6: Residuals and autocorrelation",
    "section": "Task 6.3 (Optional)",
    "text": "Task 6.3 (Optional)\nAs a sensitivity analysis, add yet one more lag (not significant in the acf) to the last model. Does that lag improve the model?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 6: Residuals and autocorrelation"
    ]
  },
  {
    "objectID": "scripts/06-practical.html#task-6-4",
    "href": "scripts/06-practical.html#task-6-4",
    "title": "Practical Session 6: Residuals and autocorrelation",
    "section": "Task 6.4 (Optional)",
    "text": "Task 6.4 (Optional)\nCarry out similar analyzes for men and for women.\n\n\nShow the code\n#save(mortagg, mortz, file = here(\"data\", \"mortagg2_case_6.RData\"))\n\n#load(here(\"data\",\"mortagg2_case_6.RData\"))",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 6: Residuals and autocorrelation"
    ]
  },
  {
    "objectID": "scripts/07-practical.html",
    "href": "scripts/07-practical.html",
    "title": "Practical Session 7: Forecasting",
    "section": "",
    "text": "Expected learning outcomes\nBy the end of this session, participants should be able to: - understand the use of forecasting in public health surveillance data - forecast the expected number of cases of a disease into the future",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#task-7-1",
    "href": "scripts/07-practical.html#task-7-1",
    "title": "Practical Session 7: Forecasting",
    "section": "Task 7.1",
    "text": "Task 7.1\nJanuary 2020: Your boss received a phone call from the Ministry of Health. She is part of a committee that is responsible for setting the alert levels for mortality in Spain for the next year (2020). Before doing so, she gives you the task to forecast the total expected number of deaths for 2020 based on the historical data up to and including 2019.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#solution-7-1",
    "href": "scripts/07-practical.html#solution-7-1",
    "title": "Practical Session 7: Forecasting",
    "section": "Help for Task 7.1",
    "text": "Help for Task 7.1\n\n\nShow the code\nggplot(data = mortz) +\n    geom_line(\n        mapping = aes(x = year_week, y = cases),\n        colour = \"black\",\n        alpha = 1.2\n    ) +\n    geom_line(\n        mapping = aes(x = year_week, y = mort_sine2cos2trendmodel$fitted),\n        colour = \"red\",\n        alpha = 1.2\n    ) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"8 weeks\") +\n    labs(x = \"Year Week\", y = \"Weekly cases\") +\n    tsa_theme + \n    theme(axis.text.x = element_text(angle = 30, hjust = 1)) \n\n\n\n\n\n\n\n\n\nThe dataset has records up to 2019-W52, and you would like to project the data (forecast) into 2020.\nFirst, create a new time series object for 2020. Note that 2020 has 53 weeks.\n\n\nShow the code\npred.df &lt;-\n    tibble(\n        year=2020,\n        week  = 1:53,\n        index = 521:(521+53-1), # add 53 weeks\n        year_week = make_yearweek(year = year, week = week),\n        sin52 = sin(2 * pi * week / 52),\n        cos52 = cos(2 * pi * week / 52),\n        sin26 = sin(2 * pi * week / 26),\n        cos26 = cos(2 * pi * week / 26),\n        pop = 47318050) %&gt;%                # Spanish pop in 2020 (1st Jan)\n    as_tsibble(index = index)\n\nview(pred.df)\n\n\nCalculate the expected values and 95% prediction intervals for each week of 2020 assuming that the Poisson regression model with trend and 2 seasonality terms based on the previous years is appropriate.\nPredict and plot the expected number of deaths per week for 2020 with prediction intervals:\n\n\nShow the code\n# apply mort_sine2cos2trendmodel to new data and predict cases with C.I. using bootstrapping.\n# bootstrapping is a statistical method where you draw random samples from your data, \n# and analyze each of this samples.\n# https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n\n\nset.seed(12589)\npred.mort &lt;- ciTools::add_ci(pred.df,\n                            mort_sine2cos2trendmodel,\n                            names=c(\"lPI\", \"uPI\"),\n                            yhatName=\"pred_cases\",\n                            response=TRUE,\n                            type=\"boot\",\n                            nSims=1000)\nhead(pred.mort, 5)\n\n\n  year week index year_week     sin52     cos52     sin26     cos26      pop\n1 2020    1   521  2020 W01 0.1205367 0.9927089 0.2393157 0.9709418 47318050\n2 2020    2   522  2020 W02 0.2393157 0.9709418 0.4647232 0.8854560 47318050\n3 2020    3   523  2020 W03 0.3546049 0.9350162 0.6631227 0.7485107 47318050\n4 2020    4   524  2020 W04 0.4647232 0.8854560 0.8229839 0.5680647 47318050\n5 2020    5   525  2020 W05 0.5680647 0.8229839 0.9350162 0.3546049 47318050\n  pred_cases      lPI       uPI\n1   9668.633 9505.031  9847.676\n2   9816.630 9649.887 10004.593\n3   9917.123 9743.460 10116.956\n4   9965.433 9788.115 10167.868\n5   9959.873 9784.202 10162.163\n\n\n\n\nShow the code\nggplot(data = pred.mort) +\n    geom_line(\n        mapping = aes(x = year_week, y = pred_cases),\n        colour = \"red\",\n        alpha = 0.7\n    ) +\n    geom_ribbon(\n        mapping = aes(x = year_week, ymin = lPI, ymax = uPI),\n        fill = \"red\",\n        alpha = 0.1\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"4 weeks\") +\n    labs(x = \"Year Week\", y = \"Predicted weekly fatalities\") +\n    tsa_theme + \n    theme(axis.text.x = element_text(angle = 30, hjust = 1)) \n\n\n\n\n\n\n\n\n\nCalculate the total number of expected deaths in 2020 in Spain.\n\n\nShow the code\n# Total predicted cases in 2020, with lPI and uPI\npred.mort %&gt;% \n  summarise(\n    pred_cases_2020 = sum(pred_cases),\n    pred_cases_2020_lPI = sum(lPI),\n    pred_cases_2020_uPI = sum(uPI)\n  )\n\n\n  pred_cases_2020 pred_cases_2020_lPI pred_cases_2020_uPI\n1        441385.8            435448.6            447514.4",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#task-7-2",
    "href": "scripts/07-practical.html#task-7-2",
    "title": "Practical Session 7: Forecasting",
    "section": "Task 7.2 (Optional)",
    "text": "Task 7.2 (Optional)\nJanuary 2021: A committee member is interested to get a better understanding of when there were periods of unusually high excess mortality and asks you to provide an analysis that highlights the time when these occurred.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#solution-7-2",
    "href": "scripts/07-practical.html#solution-7-2",
    "title": "Practical Session 7: Forecasting",
    "section": "Help for Task 7.2",
    "text": "Help for Task 7.2\nCUSUM (cumulative sum) is a graphical method that can be used to determine when there is a change in a process (all-cause mortality in this example). In TSA it can also be used to decide whether there is a need to revise the model e.g. include a covariate or there have been changes in the seasonality.\nUsing expected values from the previous regression model, you can calculate the cumulative sum of the differences between the weekly observed and expected numbers of deaths:\n\n\nShow the code\nmortz &lt;- mortz %&gt;%\n  mutate(fit_cases = mort_sine2cos2trendmodel$fit,  # get predicted cases\n         \n         # calculate differences\n         difference = cases - fit_cases,\n         cumsum_excess = cumsum(difference),\n         diff_zero = cases - mean(fit_cases),\n         cumsum_zero = cumsum(diff_zero))\n\n\nPlot this cumulative sum of the residuals.\n\n\nShow the code\nggplot(data = mortz) +\ngeom_line(\n        mapping = aes(x = year_week, y = diff_zero),\n        colour = \"green\",\n        alpha = 0.7,\n        lwd = 2\n    ) +\n    geom_line(\n        mapping = aes(x = year_week, y = cumsum_zero),\n        colour = \"orange\",\n        alpha = 0.7,\n        lwd = 2\n    ) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n    labs(x = \"Year\", y = \"Cumulative excess cases\") +\n    tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#task-7-3",
    "href": "scripts/07-practical.html#task-7-3",
    "title": "Practical Session 7: Forecasting",
    "section": "Task 7.3 (Optional)",
    "text": "Task 7.3 (Optional)\nJanuary 2021: your boss needs to inform the Ministry of Health about excess deaths so far during the first year of the pandemic.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/07-practical.html#solution-7-3",
    "href": "scripts/07-practical.html#solution-7-3",
    "title": "Practical Session 7: Forecasting",
    "section": "Help for Task 7.3",
    "text": "Help for Task 7.3\nPlot the actual number of deaths and compare with the predictions from the model based on 2010-2019.\n\n\nShow the code\n# load mortality data for 2020\nmort2020 &lt;- import(here(\"data\", \"mortagg2020.csv\"))\n\n# order mort2020 from week 1 to week 53, same as in pred.mort\nmort2020 &lt;- mort2020 %&gt;% arrange(week)\n\n# add actual deaths from 2020 in pred.mort\npred.mort &lt;- pred.mort %&gt;% \n  mutate(cases = mort2020$cases)\n\nggplot(data = pred.mort) +\n    geom_line(\n        mapping = aes(x = year_week, y = pred_cases),\n        colour = \"red\",\n        alpha = 0.7,\n        lwd = 1.2\n    ) +\n    geom_ribbon(\n        mapping = aes(x = year_week, ymin = lPI, ymax = uPI),\n        fill = \"red\",\n        alpha = 0.1\n    ) +\n    geom_line(\n        mapping = aes(x = year_week, y = cases),\n        colour = \"black\",\n        alpha = 0.7,\n        lwd = 1.2\n    ) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"4 weeks\") +\n    labs(x = \"Year Week\", y = \"Predicted weekly cases\") +\n    tsa_theme + \n    theme(axis.text.x = element_text(angle = 30, hjust = 1)) \n\n\n\n\n\n\n\n\n\nExcess deaths can be calculated as the difference of the actual number of fatalities per week and the predicted mean, or the predicted upper 95% PI.\n\n\nShow the code\npred.mort &lt;- pred.mort %&gt;%\n  mutate(\n    difference_mean = cases - pred_cases,\n    cusum_excess_mean = cumsum(difference_mean),\n    difference_uPI = cases - uPI,\n    cusum_excess_uPI = cumsum(difference_uPI)\n  )\n\n\nggplot(data = pred.mort) +\n    geom_line(\n        mapping = aes(x = year_week, y = cusum_excess_mean),\n        colour = \"orange\",\n        alpha = 0.7,\n        lwd = 2\n    ) +\n    geom_line(\n        mapping = aes(x = year_week, y = cusum_excess_uPI),\n        colour = \"blue\",\n        alpha = 0.7,\n        lwd = 2\n    ) +   \n    #scale_y_continuous(limits = c(-100, NA)) +\n    scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"4 weeks\") +\n    labs(x = \"Year\", y = \"Cumulative excess cases\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# save(list = ls(pattern = 'mort'), file = here(\"data\", \"mortagg2_case_7.RData\"))\n#load(here(\"data\",\"mortagg2_case_7.RData\"))",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 7: Forecasting"
    ]
  },
  {
    "objectID": "scripts/09-practical.html",
    "href": "scripts/09-practical.html",
    "title": "Practical Session 9: The relation between two time series",
    "section": "",
    "text": "Session inject\nSeasonal mortality is associated with varying temperatures. To explore this relationship from a time series perspective, mortality and temperature data from Aragón, an Autonomous Community in Spain, will be employed. For this exercise, you are tasked with understanding how temperatures affect mortality, and asses whether this effect is the same across the entire year or seasonal effects can be isolated.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 9: The relation between two time series"
    ]
  },
  {
    "objectID": "scripts/09-practical.html#learn-9",
    "href": "scripts/09-practical.html#learn-9",
    "title": "Practical Session 9: The relation between two time series",
    "section": "",
    "text": "assess and interpret associations with external variables\nidentify and interpret effect modification between external variables and the outcome variable in surveillance data",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 9: The relation between two time series"
    ]
  },
  {
    "objectID": "scripts/09-practical.html#task-9-1",
    "href": "scripts/09-practical.html#task-9-1",
    "title": "Practical Session 9: The relation between two time series",
    "section": "9.1 - Asses the effect of external variables",
    "text": "9.1 - Asses the effect of external variables\n\nObjective: Using the aragon dataset, assess the effect of ambient temperature (using average weekly maximum temperature) on mortality in the autonomous community of Aragón. Is this effect uniform throughout the year?\n\n\nVisualize mortality and temperatures in Aragón\nOpen the dataset aragon.csv. There you will find mean maximum temperature and mortality data for the autonomous community of Aragon by week.\n\n\nShow the code\naragon &lt;- import(here(\"data\", \"aragon.csv\")) \n\n\nConvert the data to a time series object (aragon_ts) and plot both temperature and mortality variables using ggplot and patchwork to join both plots together\n\n\nShow the code\n# Create ts dataframe with yearweek index\naragon_ts &lt;- aragon %&gt;% \n  mutate(\n    year_week = make_yearweek(year = year, week = week),\n    index = seq.int(from = 1, to = nrow(aragon))\n  ) %&gt;% \n  as_tsibble(index = index)\n\n# Temperature plot   \naragon_ts_tmax_plot &lt;- ggplot(data = aragon_ts) +\n  geom_point(mapping = aes(x = year_week, y = tmax), colour = \"blue\", alpha = 0.5) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Week\", \n       y = \"Maximum Temperature\") +\n  tsa_theme\n\n# Mortality plot\naragon_ts_cases_plot &lt;- ggplot(data = aragon_ts) +\n  geom_point(mapping = aes(x = year_week, y = cases), colour = \"green\", alpha = 0.5) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Week\", \n       y = \"Weekly case counts\") +\n  tsa_theme\n\n# Using patchwork to join the two plots\n# The \"/\" determines an above/below display\naragon_ts_two_plot &lt;- (aragon_ts_tmax_plot / aragon_ts_cases_plot) \naragon_ts_two_plot\n\n\n\n\n\n\n\n\n\nGenerate variables for sine and cosine for annual oscillation.\n\n\nShow the code\naragon_ts &lt;- aragon_ts %&gt;% \n  mutate(sin52 = sin(2 * pi * date / 52),\n         cos52 = cos(2 * pi * date / 52))\n\n\nFit a poisson regression model with a simple trend to the weekly number of deaths, accounting for seasonality.\n\n\nShow the code\naragmodel1 &lt;- glm(cases ~ index + sin52 + cos52,\n                           family = \"poisson\",\n                           data = aragon_ts)\n\n\nPlot the predicted weekly number of deaths.\n\n\nShow the code\nggplot(data = aragon_ts) +\n  geom_point(mapping = aes(x = year_week, y = cases), alpha = 0.2) +\n  geom_line(mapping = aes(x = year_week, y = aragmodel1$fitted.values), \n            colour = \"darkorange\", \n            alpha = 0.7, \n            lwd = 1.5) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Week\", \n       y = \"Weekly case counts\", \n       title = \"Cases with fitted trend\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nDiscuss how your base model fits the original data.\n\n\nDifferential effect of temperatures during the year\nWhen you plotted the weekly number of deaths in Aragon, perhaps you noticed that mortality peaks in winter; however, there are smaller peaks during the summer, too. You can account for that in two different ways:\n\nInclude two sine/cosine functions in the model (one for 52-week cycles and one for 26-week cycles)\nExplore temperature as an explanatory variable for these two peaks. We can also take into account the fact that temperature might be behaving differently as a risk factor in winter than it does in summer (sign of effect modification?)\n\nFirst, you need to create a new winter variable, defined as the time between weeks 49 and 8:\n\n\nShow the code\naragon_ts &lt;- aragon_ts %&gt;% \n    mutate(winter = (week &gt;= 49 | week &lt;= 8))\n\n\nNow you will run two models: one including temperature as a main effect (aragmodel2), and other including an interaction term with winter (aragmodel3). Compare them and decide which is better\n\n\nShow the code\n# Model with only main effect\naragmodel2 &lt;- glm(cases ~ index + sin52 + cos52 + tmax,\n                           family = \"poisson\",\n                           data = aragon_ts)\n\n# Model with temperature interaction\naragmodel3 &lt;- glm(cases ~ index + sin52 + cos52 + winter * tmax,\n                           family = \"poisson\",\n                           data = aragon_ts)\n\n\nWe compare both model’s AIC below.\n\n\nShow the code\n# Easiest, most direct way\nAIC(aragmodel2); AIC(aragmodel3)\n\n\nWe can also create a fancier table using the library gtsummary like in practical 4. We use again exponentiate = T to exponentiate coefficients and style_number() to specify the number of digits. In addition, we use one extra argument: add_glance_table() allows to add key parameters at the bottom of the table. And table_merge() allows to take several tables built with tbl_regression and put them in one big table. This makes it easy to compare models.\n\n\nShow the code\ntbl_m2 &lt;- aragmodel2 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  add_glance_table(include = c(AIC, BIC, deviance))\n\ntbl_m3 &lt;- aragmodel3 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  add_glance_table(include = c(AIC, BIC, deviance))\n\ntbl_m2m3 &lt;- \n  tbl_merge(\n    tbls = list(tbl_m2, tbl_m3),\n    tab_spanner = c(\"aragmodel2\", \"aragmodel3\")\n  )\ntbl_m2m3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\naragmodel2\n\n\naragmodel3\n\n\n\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0001\n1.0001, 1.0002\n&lt;0.001\n1.0001\n1.0001, 1.0002\n&lt;0.001\n\n\nsin52\n1.0539\n1.0435, 1.0644\n&lt;0.001\n1.0621\n1.0508, 1.0735\n&lt;0.001\n\n\ncos52\n1.1521\n1.1245, 1.1804\n&lt;0.001\n1.1721\n1.1385, 1.2066\n&lt;0.001\n\n\ntmax\n1.0035\n1.0014, 1.0056\n0.001\n1.0078\n1.0054, 1.0103\n&lt;0.001\n\n\nAIC\n4,748\n\n\n\n\n4,666\n\n\n\n\n\n\nBIC\n4,769\n\n\n\n\n4,696\n\n\n\n\n\n\nDeviance\n1,036\n\n\n\n\n950\n\n\n\n\n\n\nwinter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    FALSE\n\n\n\n\n\n\n—\n—\n\n\n\n\n    TRUE\n\n\n\n\n\n\n1.2688\n1.2020, 1.3392\n&lt;0.001\n\n\nwinter * tmax\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    TRUE * tmax\n\n\n\n\n\n\n0.9849\n0.9806, 0.9893\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nNote that winter * tmax tells R to include terms in the model for winter, tmax and for their interaction. If we just wanted to include the interaction term without its corresponding “main effects”, i.e. without winter or tmax, we would use winter:tmax instead.\nDiscuss the output of the two models. Does the interpretation change when winter is taken into account as an interaction term along with temperature?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 9: The relation between two time series"
    ]
  },
  {
    "objectID": "scripts/09-practical.html#solution-9-1",
    "href": "scripts/09-practical.html#solution-9-1",
    "title": "Practical Session 9: The relation between two time series",
    "section": "Help for Task 9.1",
    "text": "Help for Task 9.1\nOpen the dataset aragon.csv. There you will find mean maximum temperature and mortality data for the autonomous community of Aragon by week.\n\n\nShow the code\naragon &lt;- import(here(\"data\", \"aragon.csv\")) \n\n\nConvert the data to a time series and plot both variables (weekly mean maximum temperature and mortality data).\n\n\nShow the code\n# Create ts dataframe with yearweek index\naragonz &lt;- aragon %&gt;% \n  mutate(\n    year_week = make_yearweek(year = year, week = week),\n    index = seq.int(from = 1, to = nrow(aragon))\n  ) %&gt;% \n  as_tsibble(index = index)\n\n# Temperature plot   \naragonz_tmax_plot &lt;- ggplot(data = aragonz) +\n  geom_point(mapping = aes(x = year_week, y = tmax), colour = \"blue\", alpha = 0.5) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Week\", \n       y = \"Maximum Temperature\") +\n  tsa_theme\n\n# Mortality plot\naragonz_cases_plot &lt;- ggplot(data = aragonz) +\n  geom_point(mapping = aes(x = year_week, y = cases), colour = \"green\", alpha = 0.5) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Week\", \n       y = \"Weekly case counts\") +\n  tsa_theme\n\n# Using patchwork to join the two plots\n# The \"/\" determines an above/below display\naragonz_two_plot &lt;- (aragonz_tmax_plot / aragonz_cases_plot) \naragonz_two_plot\n\n\n\n\n\n\n\n\n\nGenerate variables for sine and cosine for annual oscillation.\n\n\nShow the code\naragonz &lt;- aragonz %&gt;% \n  mutate(sin52 = sin(2 * pi * date / 52),\n         cos52 = cos(2 * pi * date / 52))\n\n\nFit a poisson regression model with a simple trend to the weekly number of deaths, accounting for seasonality.\n\n\nShow the code\naragmodel1 &lt;- glm(cases ~ index + sin52 + cos52,\n                           family = \"poisson\",\n                           data = aragonz)\n\nsummary(aragmodel1)\n\n\n\nCall:\nglm(formula = cases ~ index + sin52 + cos52, family = \"poisson\", \n    data = aragonz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 5.252e+00  6.312e-03 832.117  &lt; 2e-16 ***\nindex       1.260e-04  2.082e-05   6.052 1.43e-09 ***\nsin52       4.442e-02  4.426e-03  10.037  &lt; 2e-16 ***\ncos52       1.038e-01  4.417e-03  23.500  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1726.7  on 519  degrees of freedom\nResidual deviance: 1046.5  on 516  degrees of freedom\nAIC: 4756.3\n\nNumber of Fisher Scoring iterations: 4\n\n\nPlot the predicted weekly number of deaths.\n\n\nShow the code\nggplot(data = aragonz) +\n  geom_point(mapping = aes(x = year_week, y = cases), alpha = 0.2) +\n  geom_line(mapping = aes(x = year_week, y = aragmodel1$fitted.values), \n            colour = \"darkorange\", \n            alpha = 0.7, \n            lwd = 1.5) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Week\", \n       y = \"Weekly case counts\", \n       title = \"Cases with fitted trend\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nDiscuss how your model fits the data.\nNote: When you plotted the weekly number of deaths in Aragon, perhaps you noticed that mortality peaks in winter; however, there are smaller peaks during the summer, too. You can account for that in two different ways: a) include two sine/cosine functions in the model (one for 52-week cycles and one for 26-week cycles), or b) take into account the fact that temperature might be behaving differently as a risk factor in winter than it does in summer (sign of effect modification?).\nYou decide to perform your analysis twice; once for winter and once for the rest of the time. You define winter as the time between weeks 49 and 8.\n\n\nShow the code\naragonz &lt;- aragonz %&gt;% \n    mutate(winter = (week &gt;= 49 | week &lt;= 8))\n\n\nFirst run the model including winter as a main effect, and then including an interaction term with temperature.\n\n\nShow the code\n# Model with only main effect\naragmodel2 &lt;- glm(cases ~ index + sin52 + cos52 + winter,\n                           family = \"poisson\",\n                           data = aragonz)\n\n# Model with temperature interaction\naragmodel3 &lt;- glm(cases ~ index + sin52 + cos52 + winter * tmax,\n                           family = \"poisson\",\n                           data = aragonz)\n\n\nWe compare both model’s AIC below.\n\n\nShow the code\n# Easiest, most direct way\nAIC(aragmodel2); AIC(aragmodel3)\n\n\nWe can also create a fancier table using the library gtsummary like in practical 4. We use again exponentiate = T to exponentiate coefficients and style_number() to specify the number of digits. In addition, we use one extra argument: add_glance_table() allows to add key parameters at the bottom of the table. And table_merge() allows to take several tables built with tbl_regression and put them in one big table. This makes it easy to compare models.\n\n\nShow the code\ntbl_m2 &lt;- aragmodel2 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  add_glance_table(include = c(AIC, BIC, deviance))\n\ntbl_m3 &lt;- aragmodel3 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  add_glance_table(include = c(AIC, BIC, deviance))\n\ntbl_m2m3 &lt;- \n  tbl_merge(\n    tbls = list(tbl_m2, tbl_m3),\n    tab_spanner = c(\"aragmodel2\", \"aragmodel3\")\n  )\ntbl_m2m3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\naragmodel2\n\n\naragmodel3\n\n\n\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0001\n1.0001, 1.0002\n&lt;0.001\n1.0001\n1.0001, 1.0002\n&lt;0.001\n\n\nsin52\n1.0403\n1.0311, 1.0495\n&lt;0.001\n1.0621\n1.0508, 1.0735\n&lt;0.001\n\n\ncos52\n1.0783\n1.0651, 1.0917\n&lt;0.001\n1.1721\n1.1385, 1.2066\n&lt;0.001\n\n\nwinter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    FALSE\n—\n—\n\n\n—\n—\n\n\n\n\n    TRUE\n1.0673\n1.0461, 1.0889\n&lt;0.001\n1.2688\n1.2020, 1.3392\n&lt;0.001\n\n\nAIC\n4,718\n\n\n\n\n4,666\n\n\n\n\n\n\nBIC\n4,739\n\n\n\n\n4,696\n\n\n\n\n\n\nDeviance\n1,006\n\n\n\n\n950\n\n\n\n\n\n\ntmax\n\n\n\n\n\n\n1.0078\n1.0054, 1.0103\n&lt;0.001\n\n\nwinter * tmax\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    TRUE * tmax\n\n\n\n\n\n\n0.9849\n0.9806, 0.9893\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nNote that winter * tmax tells R to include terms in the model for winter, tmax and for their interaction. If we just wanted to include the interaction term without its corresponding “main effects”, i.e. without winter or tmax, we would use winter:tmax instead.\nDiscuss the output of the two models. Does the interpretation change when winter is taken into account as an interaction term along with temperature?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 9: The relation between two time series"
    ]
  },
  {
    "objectID": "scripts/09-practical.html#task-9-2",
    "href": "scripts/09-practical.html#task-9-2",
    "title": "Practical Session 9: The relation between two time series",
    "section": "9.2 - Accounting for delayed effects (Optional)",
    "text": "9.2 - Accounting for delayed effects (Optional)\nIt has been argued by experts that low temperatures in winter might be “slow killers”; that is, very low temperatures in winter do not result in a peak in mortality on the same day/week as they are observed, but rather after some time has elapsed. On the other hand, very high temperatures in summer are “fast killers”; they are associated with peaks in mortality very fast, i.e. heat waves kill people fast. Can you find evidence for this in Aragón?\n\nDefine and explore lagged temperatures on mortality\nstats::lag is the default base R function to compute lags from the stats package. In the example below, we prefer to use one of the tidyverse package alternatives,namely dplyr::lag. We specify the package name here to avoid possible conflicts with other packages which have a lag function.\nFor a given week, the value of tmax is lagged by 1 in respect to the previous week’s value.\n\n\nShow the code\n# Create lag variable\naragon_ts &lt;- aragon_ts %&gt;% \n  mutate(L1.tmax = dplyr::lag(x = tmax, n = 1))\n\n\nCreate a new model named aragmodel4 including the lag variable, and compare it with the previous model aragmodel3. What can you conclude from it?\n\n\nShow the code\naragmodel4 &lt;- glm(cases ~ index + sin52 + cos52 + winter * tmax + L1.tmax,\n                           family = \"poisson\",\n                           data = aragon_ts)\n\ntbl_m4 &lt;- aragmodel4 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  add_glance_table(include = c(AIC, BIC, deviance))\n\ntbl_m3m4 &lt;- \n  tbl_merge(\n    tbls = list(tbl_m3, tbl_m4),\n    tab_spanner = c(\"aragmodel3\", \"aragmodel4\")\n  )\ntbl_m3m4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\naragmodel3\n\n\naragmodel4\n\n\n\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0001\n1.0001, 1.0002\n&lt;0.001\n1.0001\n1.0001, 1.0002\n&lt;0.001\n\n\nsin52\n1.0621\n1.0508, 1.0735\n&lt;0.001\n1.0584\n1.0454, 1.0717\n&lt;0.001\n\n\ncos52\n1.1721\n1.1385, 1.2066\n&lt;0.001\n1.1623\n1.1250, 1.2008\n&lt;0.001\n\n\nwinter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    FALSE\n—\n—\n\n\n—\n—\n\n\n\n\n    TRUE\n1.2688\n1.2020, 1.3392\n&lt;0.001\n1.2546\n1.1876, 1.3252\n&lt;0.001\n\n\ntmax\n1.0078\n1.0054, 1.0103\n&lt;0.001\n1.0082\n1.0057, 1.0108\n&lt;0.001\n\n\nwinter * tmax\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    TRUE * tmax\n0.9849\n0.9806, 0.9893\n&lt;0.001\n0.9858\n0.9814, 0.9902\n&lt;0.001\n\n\nAIC\n4,666\n\n\n\n\n4,654\n\n\n\n\n\n\nBIC\n4,696\n\n\n\n\n4,688\n\n\n\n\n\n\nDeviance\n950\n\n\n\n\n944\n\n\n\n\n\n\nL1.tmax\n\n\n\n\n\n\n0.9988\n0.9965, 1.0010\n0.3\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\n\n\nWinter vs No-winter\nThe lagged effect of temperature was not signifcant in aragmodel4 but you are still curious about the differential effect of temperatures on mortality in winter and the other seasons. Instead of running the model with another interaction term, you could run the analyses for winter and the rest of your dataset separately.\nFirst, create aragon_ts.winter and aragon_ts.notwinter\n\n\nShow the code\n## winter subset\naragon_ts.winter &lt;- \n    aragon_ts %&gt;% \n    filter(winter == TRUE)\n\n## not winter subset\naragon_ts.notwinter &lt;- \n    aragon_ts %&gt;% \n    filter(winter == FALSE)\n\n\nNow, run two new models aragmodel5 for the winter data and aragmodel6 for non-winter data. Think carefully about how you will compare them: is the AIC/BIC criteria still valid for this task?\n\n\nShow the code\n# Winter model\naragmodel5 &lt;- glm(cases ~ index + sin52 + cos52 + L1.tmax,\n                           family = \"poisson\",\n                           data = aragon_ts.winter)\n\n# Not winter model\naragmodel6 &lt;- glm(cases ~ index + sin52 + cos52 + L1.tmax,\n                           family = \"poisson\",\n                           data = aragon_ts.notwinter)\n\n# Table summaries\ntbl_m5 &lt;- aragmodel5 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4))\n\ntbl_m6 &lt;- aragmodel6 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4))\n\n# Comparison table\ntbl_m5m6 &lt;- \n  tbl_merge(\n    tbls = list(tbl_m5, tbl_m6),\n    tab_spanner = c(\"Winter model\", \"Not winter model\")\n  )\ntbl_m5m6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nWinter model\n\n\nNot winter model\n\n\n\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0002\n1.0001, 1.0003\n&lt;0.001\n1.0001\n1.0001, 1.0002\n&lt;0.001\n\n\nsin52\n1.1944\n1.1428, 1.2486\n&lt;0.001\n1.0485\n1.0348, 1.0624\n&lt;0.001\n\n\ncos52\n1.7229\n1.4361, 2.0675\n&lt;0.001\n1.1094\n1.0785, 1.1412\n&lt;0.001\n\n\nL1.tmax\n0.9923\n0.9879, 0.9967\n&lt;0.001\n1.0031\n1.0006, 1.0055\n0.015\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nDiscuss the output of the different models. Which one would you go for?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 9: The relation between two time series"
    ]
  },
  {
    "objectID": "scripts/09-practical.html#solution-9-2",
    "href": "scripts/09-practical.html#solution-9-2",
    "title": "Practical Session 9: The relation between two time series",
    "section": "Help for Task 9.2",
    "text": "Help for Task 9.2\nIt has been argued by experts that low temperatures in winter might be “slow killers”; that would mean that very low temperatures in winter do not result in a peak in mortality on the same day/week when they are observed, but rather after some time. On the other hand, very high temperatures in summer are fast killers; they are associated with peaks in mortality very fast, i.e. heat waves kill people fast.\nFor this reason, you decide to check whether temperature has an effect on mortality with some lag. stats::lag is the default base R function to compute lags from the stats package. In the example below, we prefer to use one of the tidyverse package alternatives,namely dplyr::lag. We specify the package name here to avoid possible conflicts with other packages which have a lag function.\nWe are using the lag function from the dplyr package. For a given week, the value of tmax is lagged by 1 in respect to the previous week’s value.\n\n\nShow the code\n# Create lag variable\naragonz &lt;- aragonz %&gt;% \n  mutate(L1.tmax = dplyr::lag(x = tmax, n = 1))\n\n\nCreate a new model named aragmodel4 including the lag variable, and compare it with the previous model aragmodel3. What can you conclude from it?\n\n\nShow the code\naragmodel4 &lt;- glm(cases ~ index + sin52 + cos52 + winter * tmax + L1.tmax,\n                           family = \"poisson\",\n                           data = aragonz)\n\nsummary(aragmodel4)\n\n\n\nCall:\nglm(formula = cases ~ index + sin52 + cos52 + winter * tmax + \n    L1.tmax, family = \"poisson\", data = aragonz)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      5.087e+00  3.198e-02 159.076  &lt; 2e-16 ***\nindex            1.246e-04  2.097e-05   5.942 2.81e-09 ***\nsin52            5.679e-02  6.343e-03   8.954  &lt; 2e-16 ***\ncos52            1.504e-01  1.663e-02   9.043  &lt; 2e-16 ***\nwinterTRUE       2.268e-01  2.795e-02   8.113 4.94e-16 ***\ntmax             8.194e-03  1.304e-03   6.283 3.32e-10 ***\nL1.tmax         -1.226e-03  1.146e-03  -1.070    0.285    \nwinterTRUE:tmax -1.433e-02  2.278e-03  -6.291 3.15e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1708.45  on 518  degrees of freedom\nResidual deviance:  943.72  on 511  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 4654.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nShow the code\nAIC(aragmodel3); AIC(aragmodel4)\n\n\n[1] 4665.836\n\n\n[1] 4654.111\n\n\nInstead of running the model with an interaction term, you could run the analyses for winter and the rest of your dataset separately.\nThe interaction term was not significant, but you are still curious about the differential effect of temperatures on mortality in winter and the other seasons. You decide to split the data and run two different models for winter and non-winter mortality and temperatures, and compare them.\nThink carefully about how you will compare them: are theAIC criteria still valid for this task?\n\n\nShow the code\n## winter subset\naragonz.winter &lt;- \n    aragonz %&gt;% \n    filter(winter == TRUE)\n\n## not winter subset\naragonz.notwinter &lt;- \n    aragonz %&gt;% \n    filter(winter == FALSE)\n\n\n\n\nShow the code\n# Winter model\naragmodel5 &lt;- glm(cases ~ index + sin52 + cos52 + L1.tmax,\n                           family = \"poisson\",\n                           data = aragonz.winter)\n\n# Not winter model\naragmodel6 &lt;- glm(cases ~ index + sin52 + cos52 + L1.tmax,\n                           family = \"poisson\",\n                           data = aragonz.notwinter)\n\n# Table summaries\ntbl_m5 &lt;- aragmodel5 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4))\n\ntbl_m6 &lt;- aragmodel6 %&gt;% \n  tbl_regression(exponentiate = TRUE,\n                 estimate_fun = ~style_number(.x, digits = 4))\n\n# Comparison table\ntbl_m5m6 &lt;- \n  tbl_merge(\n    tbls = list(tbl_m5, tbl_m6),\n    tab_spanner = c(\"Winter model\", \"Not winter model\")\n  )\ntbl_m5m6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nWinter model\n\n\nNot winter model\n\n\n\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0002\n1.0001, 1.0003\n&lt;0.001\n1.0001\n1.0001, 1.0002\n&lt;0.001\n\n\nsin52\n1.1944\n1.1428, 1.2486\n&lt;0.001\n1.0485\n1.0348, 1.0624\n&lt;0.001\n\n\ncos52\n1.7229\n1.4361, 2.0675\n&lt;0.001\n1.1094\n1.0785, 1.1412\n&lt;0.001\n\n\nL1.tmax\n0.9923\n0.9879, 0.9967\n&lt;0.001\n1.0031\n1.0006, 1.0055\n0.015\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nDiscuss the output of the different models. Which one would you go for?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 9: The relation between two time series"
    ]
  },
  {
    "objectID": "scripts/10-practical.html",
    "href": "scripts/10-practical.html",
    "title": "Practical Session 10: Assessing the impact of interventions using surveillance data",
    "section": "",
    "text": "Expected learning outcomes\nBy the end of the case study, participants will be able to:",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 10: Assessing the impact of interventions using surveillance data"
    ]
  },
  {
    "objectID": "scripts/10-practical.html#learn-10",
    "href": "scripts/10-practical.html#learn-10",
    "title": "Practical Session 10: Assessing the impact of interventions using surveillance data",
    "section": "",
    "text": "Assess and interpret the effect of an intervention on trends and periodicity in surveillance data",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 10: Assessing the impact of interventions using surveillance data"
    ]
  },
  {
    "objectID": "scripts/10-practical.html#description-of-the-dataset",
    "href": "scripts/10-practical.html#description-of-the-dataset",
    "title": "Practical Session 10: Assessing the impact of interventions using surveillance data",
    "section": "Description of the dataset",
    "text": "Description of the dataset\nRotavirus is a very common and potentially serious infection of the large bowel, mainly affecting young babies. Nearly every child will have at least one episode of rotavirus gastroenteritis by five years of age. People of any age can be affected, but the illness is more severe in young infants. The rotavirus immunisation programme was introduced in the UK on 1 July 2013 (week 27) with the objective of preventing a significant number of young infants from developing rotavirus infection. It may also provide some additional protection to the wider population through herd immunity. The aim of the rotavirus immunisation programme is to provide two doses of vaccine to infants from six weeks of age and before 24 weeks of age. The first dose of vaccine is offered at approximately eight weeks of age and the second dose at least four weeks after the first dose.\nHigh coverage was rapidly achieved for the first cohort of children offered rotavirus vaccine routinely in England and this has been maintained throughout the first fourteen months of the routine programme. Over this period, rotavirus vaccine coverage for children in the routine cohort averaged 93.3% for one dose and 88.3% for two doses.1",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 10: Assessing the impact of interventions using surveillance data"
    ]
  },
  {
    "objectID": "scripts/10-practical.html#task-10-1",
    "href": "scripts/10-practical.html#task-10-1",
    "title": "Practical Session 10: Assessing the impact of interventions using surveillance data",
    "section": "Task 10.1",
    "text": "Task 10.1\nYou have been given a dataset for notified Rotavirus infections in England and Wales from week 23/2009 up to week 42/2018 (rotavirus.csv).\nAssess the impact of the introduction of the vaccination scheme in England and Wales for the whole population. Has the vaccine introduction had an effect on trend or periodicity patterns? Has the vaccine introduction affected the timing of the yearly outbreaks?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 10: Assessing the impact of interventions using surveillance data"
    ]
  },
  {
    "objectID": "scripts/10-practical.html#solution-10-1",
    "href": "scripts/10-practical.html#solution-10-1",
    "title": "Practical Session 10: Assessing the impact of interventions using surveillance data",
    "section": "Help for Task 10.1",
    "text": "Help for Task 10.1\nOpen the rotavirus dataset, which is a line list of all notified rotavirus infections in the UK between June 2009 and December 2014. Prepare your data and test if the introduction of the vaccine has had an impact on the notified number of cases of rotavirus infections in the UK. Has the trend changed? Has the periodicity changed?\nPrepare the rotavirus.csv for time-series analysis. You need to manipulate the dataset before being able to use it for time series analysis.\n\n\nShow the code\nrota &lt;- import(here(\"data\", \"rotavirus.csv\")) \nview(rota)\n\n\nAs the data in rota shows above, this dataset is structured in a “wide” format. You can see two variables representing dates, and all other variables containing case counts are organised horizontally. Alternatively, we can store the same information by converting the first dataset to a “long” format, where one variable contains the different age group categories of cases (cases_cat), and another displays the corresponding case counts (n) for each age group and year-week.\nIn order to use ggplot2, remember that the data should be organised in a long format.\nThe function as_tsibble() is what indicates to R that the data is a time series. index represents the time variable and key represents the outcome studied.\n\n\nShow the code\n## convert to long format\nrota_lg &lt;- rota %&gt;% \n    pivot_longer(\n      cols = -c(year, week),\n      names_to = \"cases_cat\",\n      values_to = \"n\"\n    )\n\n## recode\nrota_lg &lt;- rota_lg %&gt;%\n  mutate(cases_cat = case_when(\n    cases_cat == \"case1\" ~ \"Cases &lt;1 year\",\n    cases_cat == \"case2\" ~ \"Cases 1 to 4 years\",\n    cases_cat == \"case3\" ~ \"Cases 5 to 14 years\",\n    cases_cat == \"case4\" ~ \"Cases 15 to 64 years\",\n    cases_cat == \"case5\" ~ \"Cases 65+ years\",\n    cases_cat == \"cases\" ~ \"Total cases\",\n    TRUE ~ NA_character_\n    ))\n\n## turn into ts format\nrota_lg &lt;- rota_lg %&gt;%\n  mutate(year_week = make_yearweek(year = year, week = week)) %&gt;%\n  group_by(cases_cat) %&gt;%\n  mutate(index = seq.int(from = 1, to = length(unique(year_week)))) %&gt;%\n  ungroup() %&gt;%\n  as_tsibble(index = year_week, key = cases_cat)\n\n\nPlot the total number of cases by age group against time. Note that .\n\n\nShow the code\nggplot(data = rota_lg) +\n  geom_point(mapping = aes(x = year_week, y = n), alpha = 0.3) +\n  facet_wrap(facets = vars(cases_cat), \n             scales = \"free_y\") +   \n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(\n    x = \"Year\", \n    y = \"Weekly case counts\",\n    title = \"Rotavirus cases\\n(note differing y scales)\"\n  ) +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nShow the code\n## Notes: \n# You can get line breaks in labels using \\n\n# You can also make all y-axes the same by using scales = \"fixed_y\"\n\n\nWe want to see whether the number of cases changed after the introduction of the vaccine.\nFor that, first we need to state in the dataset when the vaccine was available. Firstly, we are adding one new vaccine variable to the rota_ts time series. It will be 0 for weeks before week 27 2013, and 1 thereafter.\nWe have also created sine and cosine variables so that we can model annual seasonality as before.\n\n\nShow the code\nrota_ts &lt;- rota_lg %&gt;%\n  mutate(\n    vaccine = case_when(\n      year &gt;= 2014 ~ 1,\n      year &lt;= 2012 ~ 0,\n      year == 2013 & week &gt;= 27 ~ 1,\n      year == 2013 & week &lt; 27 ~ 0,\n      TRUE ~ NA_real_\n      ),\n    sin52 = sin(2 * pi * index / 52),\n    cos52 = cos(2 * pi * index / 52)\n  )\n\n## subset to total cases\nrota_ts &lt;- rota_ts %&gt;% \n  filter(cases_cat == \"Total cases\")\n\n\nRun first a separate model for the time before and after the introduction of the vaccine in England and Wales for all population. Can you detect any change in trend?\nNote in the code that we are building further on the tbl_regression function, adding options.\nWhat we saw before:\n\nexponentiate = T to exponentiate coefficients\nstyle_number() to specify the number of digits in the estimates.\nadd_glance_table() to add key parameters at the bottom of the table.\ntable_merge() to take several tables built with tbl_regression and put them in one big table.\n\nWhat we are adding now: - style_pvalue to format the pvalue (here with max 3 digits) - modify_caption to add a table title\n\n\nShow the code\nrotamodel_prevacc_p &lt;- glm(n ~ index,\n    data = rota_ts %&gt;% filter(vaccine == 0),\n    family = \"poisson\"\n) \n\n#If the `summary` command is used for a `glm` Poisson model, the log rate ratios are reported by \n#default. They need to be exponentiated \n\nsummary_rotamodel_prevacc_p &lt;- rotamodel_prevacc_p %&gt;% \n  gtsummary::tbl_regression(\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 3), \n    estimate_fun = ~style_number(.x, digits = 4)\n    ) %&gt;%\n  gtsummary::add_glance_table(include = c(AIC, deviance, df.residual)) %&gt;% \n  gtsummary::modify_caption(\"Rotavirus model pre-vaccination - Poisson\")\n  \nsummary_rotamodel_prevacc_p\n\n\n\n\n\n\nRotavirus model pre-vaccination - Poisson\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0028\n1.0027, 1.0029\n&lt;0.001\n\n\nAIC\n78,347\n\n\n\n\n\n\nDeviance\n76,884\n\n\n\n\n\n\nResidual df\n210\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nIs a Poisson model the best option?\nBefore we have a look at the period post-vaccination, is a Poisson model really the best model to use here? We need to check whether our data exhibit overdispersion.\nOverdispersion occurs when the mean of the data is not equal to its variance, violating the fundamental assumption of the Poisson distribution. If this happens, the model underestimates the standard error of the coefficients, biasing results towards narrower confidence intervals and artifically lower p-values.\nLet’s check for overdispersion. We can calculate an overdispersion parameter from a model as the ratio of the deviance and the residuals degrees of freedom. Also, proper statistical test are available, for example using the AER library and the dispersiontest function\n\n\nShow the code\n# Quick check for overdispersion: if check &gt; 1 then use negative binomial\ndeviance &lt;- summary(rotamodel_prevacc_p)$deviance\nresidual.df &lt;- summary(rotamodel_prevacc_p)$df.residual\n\ncheck &lt;- deviance / residual.df\ncheck\n\n\n[1] 366.1138\n\n\nShow the code\n# Proper check for overdispersion: \nAER::dispersiontest(rotamodel_prevacc_p, trafo = NULL, alternative = c(\"greater\"))\n\n\n\n    Overdispersion test\n\ndata:  rotamodel_prevacc_p\nz = 7.2434, p-value = 2.188e-13\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n   446.172 \n\n\nThe overdispersion parameter is fairly greater that 1, and the overdispersion test is significant, so we can safely conclude that a negative binomial model is more appropiate. Let’s compute it with the glm.nb function, and compare with the previous one\n\n\nShow the code\n#so best to use a negative binomial here\nrotamodel_prevacc_nb &lt;- glm.nb(n ~ index,\n                               data = rota_ts %&gt;% filter(vaccine == 0)) \n\n\n# Table with results\nsummary_rotamodel_prevacc_nb &lt;- rotamodel_prevacc_nb %&gt;%\n  gtsummary::tbl_regression(\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 3),\n    estimate_fun = ~style_number(.x, digits = 4)\n    ) %&gt;%\n  gtsummary::add_glance_table(include = c(AIC, deviance, df.residual)) %&gt;% \n  gtsummary::modify_caption(\"Rotavirus model pre-vaccination - Negative Binomial\")\n  \n\n# Let's compare the two models\ncomparison_p_nb &lt;- gtsummary::tbl_merge(\n  tbls = list(summary_rotamodel_prevacc_p, \n              summary_rotamodel_prevacc_nb),\n  tab_spanner = c(\"Poisson model\", \"Negative Binomial model\")\n  )\n\ncomparison_p_nb\n\n\n\n\n\n\nRotavirus model pre-vaccination - Poisson\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nPoisson model\n\n\nNegative Binomial model\n\n\n\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0028\n1.0027, 1.0029\n&lt;0.001\n1.0029\n1.0005, 1.0054\n0.015\n\n\nAIC\n78,347\n\n\n\n\n2,854\n\n\n\n\n\n\nDeviance\n76,884\n\n\n\n\n248\n\n\n\n\n\n\nResidual df\n210\n\n\n\n\n210\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nThe AIC comparison proves how the negative binomial model outperforms the Poisson model. Also, realize how the index coefficient shifted from a tiny negative to a tiny positive overal trend, and how the p-value has greatly increased (but it still significant)\nLet’s use negative binomial models for the rest of the practical.\nNow let’s look at the period post-introduction of the vaccine.\nWe will fit a model for the post-vaccination period and compare it with the pre-vaccination period\n\n\nShow the code\nrotamodel_postvacc &lt;- glm.nb(n ~ index,\n                             data = rota_ts %&gt;% filter(vaccine == 1))\n\n# Table with results \nsummary_rotamodel_postvacc &lt;- rotamodel_postvacc %&gt;% \n  gtsummary::tbl_regression(\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 3),\n    estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  gtsummary::add_glance_table(include = c(AIC, deviance, df.residual)) %&gt;% \n  gtsummary::modify_caption(\"Rotavirus model post-vaccination\")\n\n\n# Comparisson between models\ncomparison_pre_post &lt;- gtsummary::tbl_merge(\n    tbls = list(summary_rotamodel_prevacc_nb, \n                summary_rotamodel_postvacc),\n    tab_spanner = c(\"Pre-vaccination\", \"Post-vaccination\")\n  )\n\ncomparison_pre_post\n\n\n\n\n\n\nRotavirus model pre-vaccination - Negative Binomial\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nPre-vaccination\n\n\nPost-vaccination\n\n\n\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0029\n1.0005, 1.0054\n0.015\n0.9977\n0.9967, 0.9988\n&lt;0.001\n\n\nAIC\n2,854\n\n\n\n\n2,830\n\n\n\n\n\n\nDeviance\n248\n\n\n\n\n293\n\n\n\n\n\n\nResidual df\n210\n\n\n\n\n274\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nWe can see a difference in trends, but the two models cannot be directly compared.\nNow we want to run one single model for both periods (before/after the introduction of the vaccine)\n\n\nShow the code\nrotamodel_trend &lt;- glm.nb(n ~ index + vaccine,\n                          data = rota_ts)\n\n# Table with results\nsummary_rotamodel_trend &lt;- rotamodel_trend %&gt;% \n  gtsummary::tbl_regression(\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 3),\n    estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  gtsummary::add_glance_table(include = c(AIC, deviance, df.residual)) %&gt;% \n  gtsummary::modify_caption(\"Rotavirus model with trend and vaccine intervention\")\n\nsummary_rotamodel_trend\n\n\n\n\n\n\nRotavirus model with trend and vaccine intervention\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nindex\n0.9995\n0.9983, 1.0006\n0.341\n\n\nvaccine\n0.2624\n0.1940, 0.3551\n&lt;0.001\n\n\nAIC\n5,768\n\n\n\n\n\n\nDeviance\n544\n\n\n\n\n\n\nResidual df\n485\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nHow would you interpret these results?\nWe can plot the model against the raw data.\n\n\nShow the code\nrota_ts &lt;- rota_ts %&gt;% \n  mutate(fitted_trend = fitted(rotamodel_trend))\n   \n\nggplot(data = rota_ts) +\n  geom_point(mapping = aes(x = year_week, y = n), alpha = 0.3) +\n  geom_line(mapping = aes(x = year_week, y = fitted_trend), \n            alpha = 0.7, colour = \"green\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Year\", \n       y = \"Weekly case counts\",\n       title = \"Rotavirus cases: model with trend and intervention\"\n  ) +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nNow we want to run one single model for both periods (before/after the introduction of the vaccine), this time including seasonality.\n\n\nShow the code\nrotamodel_seasonality &lt;- glm.nb(n ~ index + sin52 + cos52 + vaccine,\n                                data = rota_ts)\n\n\nsummary_rotamodel_seasonality &lt;- rotamodel_seasonality %&gt;% \n  gtsummary::tbl_regression(\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 3),\n    estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  gtsummary::add_glance_table(include = c(AIC, deviance, df.residual)) %&gt;% \n  gtsummary::modify_caption(\"Rotavirus model with seasonality, trend and vaccine intervention\")\n\nsummary_rotamodel_seasonality\n\n\n\n\n\n\nRotavirus model with seasonality, trend and vaccine intervention\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nindex\n0.9989\n0.9982, 0.9996\n0.001\n\n\nsin52\n0.4245\n0.3951, 0.4560\n&lt;0.001\n\n\ncos52\n2.0527\n1.9092, 2.2071\n&lt;0.001\n\n\nvaccine\n0.4885\n0.4034, 0.5919\n&lt;0.001\n\n\nAIC\n5,225\n\n\n\n\n\n\nDeviance\n506\n\n\n\n\n\n\nResidual df\n483\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nWe can plot the model against the raw data again.\n\n\nShow the code\nrota_ts &lt;- rota_ts %&gt;% \n  mutate(fitted_seasonality = fitted(rotamodel_seasonality))\n \n\nggplot(data = rota_ts) + \n  geom_point(mapping = aes(x = year_week, y = n), alpha = 0.3) +\n  geom_line(mapping = aes(x = year_week, y = fitted_seasonality), \n            alpha = 0.7, colour = \"green\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(x = \"Year\", \n       y = \"Weekly case counts\",\n       title = \"Rotavirus cases: model with trend, seasonality, and intervention\"\n    ) +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nTo be able to comment on any change in trend after the introduction of the vaccine, we need to either include an interaction term in the model or include a time variable that starts at the introduction of the vaccine. Let’s here use an interaction term.\n\n\nShow the code\nrotamodel_winteraction &lt;- glm.nb(n ~ index * vaccine + sin52 + cos52,\n                                 data = rota_ts)\n\nsummary_rotamodel_trendinteraction &lt;- rotamodel_winteraction %&gt;% \n  gtsummary::tbl_regression(\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 3),\n    estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  gtsummary::add_glance_table(include = c(AIC, deviance, df.residual)) %&gt;% \n  gtsummary::modify_caption(\"Rotavirus model with seasonality, trend, vaccine and interaction between intervention and trend\")\n\nsummary_rotamodel_trendinteraction\n\n\n\n\n\n\nRotavirus model with seasonality, trend, vaccine and interaction between intervention and trend\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nindex\n1.0015\n1.0003, 1.0027\n0.015\n\n\nvaccine\n0.9833\n0.7076, 1.3693\n0.919\n\n\nsin52\n0.4290\n0.4001, 0.4600\n&lt;0.001\n\n\ncos52\n2.0434\n1.9036, 2.1935\n&lt;0.001\n\n\nindex * vaccine\n0.9962\n0.9948, 0.9977\n&lt;0.001\n\n\nAIC\n5,202\n\n\n\n\n\n\nDeviance\n505\n\n\n\n\n\n\nResidual df\n482\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nYou can visualize the effect of the interaction term using plot_model() from the sjPlot library\n\n\nShow the code\nsjPlot::plot_model(rotamodel_winteraction, type =\"int\") \n\n\n\n\n\n\n\n\n\nHave there been any changes in seasonality, too?\nNote how we can use brackets to introduce interaction terms for several variables.\n\n\nShow the code\nrotamodel_full &lt;- glm.nb(n ~ (sin52 + cos52 + index)*vaccine,\n                         data = rota_ts)\n\n\nsummary_rotamodel_full &lt;- rotamodel_full %&gt;% \n  gtsummary::tbl_regression(\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 3),\n    estimate_fun = ~style_number(.x, digits = 4)) %&gt;%\n  gtsummary::add_glance_table(include = c(AIC, deviance, df.residual)) %&gt;% \n  gtsummary::modify_caption(\"Rotavirus model with seasonality, trend, vaccine and interaction term\")\n\nsummary_rotamodel_full\n\n\n\n\n\n\nRotavirus model with seasonality, trend, vaccine and interaction term\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nsin52\n0.2551\n0.2361, 0.2756\n&lt;0.001\n\n\ncos52\n2.1715\n1.9979, 2.3602\n&lt;0.001\n\n\nindex\n1.0006\n0.9997, 1.0016\n0.185\n\n\nvaccine\n0.9175\n0.7086, 1.1897\n0.504\n\n\nsin52 * vaccine\n2.6312\n2.3698, 2.9216\n&lt;0.001\n\n\ncos52 * vaccine\n0.8702\n0.7800, 0.9709\n0.010\n\n\nindex * vaccine\n0.9971\n0.9959, 0.9982\n&lt;0.001\n\n\nAIC\n4,945\n\n\n\n\n\n\nDeviance\n496\n\n\n\n\n\n\nResidual df\n480\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nShow the code\nsjPlot::plot_model(rotamodel_full, type =\"int\")\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\nLet’s plot the full model estimate.\n\n\nShow the code\nrota_ts &lt;- rota_ts %&gt;% \n  mutate(fitted_full = fitted(rotamodel_full))\n\n\nggplot(data = rota_ts) +\n  geom_point(mapping = aes(x = year_week, y = n), alpha = 0.3) +\n  geom_line(mapping = aes(x = year_week, y = fitted_full), \n            alpha = 0.7, colour = \"green\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  labs(x = \"Year\",\n       y = \"Weekly case counts\",\n       title = \"Rotavirus cases: model with trend, seasonality, intervention \\n and interaction terms\"\n  ) +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nWe can now compare all models next to each other:\n\n\nShow the code\ncomparison_all &lt;- gtsummary::tbl_merge(\n    tbls = list(summary_rotamodel_seasonality,\n                summary_rotamodel_trendinteraction,  \n                summary_rotamodel_full),\n    tab_spanner = c(\"Model without interaction terms\", \n                    \"Interaction with trend only\", \n                    \"Full model\")\n  )\n\ncomparison_all\n\n\n\n\n\n\nRotavirus model with seasonality, trend and vaccine intervention\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel without interaction terms\n\n\nInteraction with trend only\n\n\nFull model\n\n\n\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\nIRR\n95% CI\np-value\n\n\n\n\nindex\n0.9989\n0.9982, 0.9996\n0.001\n1.0015\n1.0003, 1.0027\n0.015\n1.0006\n0.9997, 1.0016\n0.185\n\n\nsin52\n0.4245\n0.3951, 0.4560\n&lt;0.001\n0.4290\n0.4001, 0.4600\n&lt;0.001\n0.2551\n0.2361, 0.2756\n&lt;0.001\n\n\ncos52\n2.0527\n1.9092, 2.2071\n&lt;0.001\n2.0434\n1.9036, 2.1935\n&lt;0.001\n2.1715\n1.9979, 2.3602\n&lt;0.001\n\n\nvaccine\n0.4885\n0.4034, 0.5919\n&lt;0.001\n0.9833\n0.7076, 1.3693\n0.919\n0.9175\n0.7086, 1.1897\n0.504\n\n\nAIC\n5,225\n\n\n\n\n5,202\n\n\n\n\n4,945\n\n\n\n\n\n\nDeviance\n506\n\n\n\n\n505\n\n\n\n\n496\n\n\n\n\n\n\nResidual df\n483\n\n\n\n\n482\n\n\n\n\n480\n\n\n\n\n\n\nindex * vaccine\n\n\n\n\n\n\n0.9962\n0.9948, 0.9977\n&lt;0.001\n0.9971\n0.9959, 0.9982\n&lt;0.001\n\n\nsin52 * vaccine\n\n\n\n\n\n\n\n\n\n\n\n\n2.6312\n2.3698, 2.9216\n&lt;0.001\n\n\ncos52 * vaccine\n\n\n\n\n\n\n\n\n\n\n\n\n0.8702\n0.7800, 0.9709\n0.010\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nWhat about age? Can you see a difference in impact of vaccination depending on age category?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 10: Assessing the impact of interventions using surveillance data"
    ]
  },
  {
    "objectID": "scripts/10-practical.html#task-10-2",
    "href": "scripts/10-practical.html#task-10-2",
    "title": "Practical Session 10: Assessing the impact of interventions using surveillance data",
    "section": "Task 10.2 (Optional)",
    "text": "Task 10.2 (Optional)\nDiscuss with colleagues how you can determine the number of cases prevented as a result of the introduction of the vaccine (Hint: refer to Practical 7!).",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 10: Assessing the impact of interventions using surveillance data"
    ]
  },
  {
    "objectID": "scripts/10-practical.html#footnotes",
    "href": "scripts/10-practical.html#footnotes",
    "title": "Practical Session 10: Assessing the impact of interventions using surveillance data",
    "section": "",
    "text": "Rotavirus infant immunisation programme 2014/15: Vaccine uptake report for England (Published: June 2015; PHE publications gateway number: 2015141)↩︎",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 10: Assessing the impact of interventions using surveillance data"
    ]
  },
  {
    "objectID": "scripts/03-practical.html",
    "href": "scripts/03-practical.html",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "",
    "text": "Session inject\nYou have been provided with one MS Excel file (tsa_practice.xlsx) containing 2 sheets, one for each of two different diseases (dis1, dis2); and one csv file (tsa_pumala.csv) about Puumala virus infections in Finland.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "scripts/03-practical.html#learning-3",
    "href": "scripts/03-practical.html#learning-3",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "",
    "text": "Expected Learning Outcomes\nBy the end of the session, participants should be able to:\n\nManage surveillance datasets with different date formats\nCreate specific time series objects using tsibble\nPlot surveillance data against time\n\n\n\nSource code\n\n\nShow the code\n# Install pacman if not installed already, and activate it\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n# Install, update and activate libraries\npacman::p_load(\n  here, \n  rio, \n  skimr,\n  tsibble,\n  ISOweek,\n  tidyverse\n)",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "scripts/03-practical.html#visualization-exercise",
    "href": "scripts/03-practical.html#visualization-exercise",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "3.1 - Visualization exercise",
    "text": "3.1 - Visualization exercise\n\nObjective: to assess visually the reported number of cases of dis1 by ISO (epidemiological) week or calendar month for all the data provided. What diseases do you think dis1 is, judging from the cases’ distribution in time?\n\n\nImport and explore data\nBegin with importing your data to R from these Excel/CSV files and examine them using the well-known str, summary and skim functions\n\n\nShow the code\ndis1 &lt;- import(here(\"data\", \"tsa_practice.xlsx\"), which = \"dis1\")\n\n\nSince we will work with time series data, we could use some time to explore the most relevant variable: date (in this case, the year variable). dis1 is a dataset containing weekly counts of a certain disease between 1981 and 1989. A typical year has 52 weeks, thus we can count the number of times each year appear in the data using table, tabyl or count function, your go-to tools for categorical variables\n\n\nShow the code\ntable(dis1$year)\n\n\n\n1981 1982 1983 1984 1985 1986 1987 1988 1989 \n  52   52   52   52   52   52   52   52   15 \n\n\n\n\nShow the code\njanitor::tabyl(dis1$year)\n\n\n dis1$year  n    percent\n      1981 52 0.12064965\n      1982 52 0.12064965\n      1983 52 0.12064965\n      1984 52 0.12064965\n      1985 52 0.12064965\n      1986 52 0.12064965\n      1987 52 0.12064965\n      1988 52 0.12064965\n      1989 15 0.03480278\n\n\nEach one provides different outputs and possesses unique capabilities. In general, tabyl should be your preferred basic function, thanks to its tidy integration and format output. It also enables some customization and data manipulation.\n\n\nTime series data format\nR has a collection of packages for handling time series data. Some of these packages are part of the tidyverse family of packages. Particularly, it includes the tsibble package, which intends to create a data infrastructure for easier manipulation and handling of temporal data, and adapts the principles of tidy data).\nAccording to the help description, a tsibble object is defined by\n\nAn index, as a variable with inherent ordering from past to present. Mandatory for a ts object\nA key, as a set of variables that define observational units over time, i.e. region, state, age group, etc.. Optional for a ts object\nUniquely identified observations by an index and a key (if defined).\n\nTo convert the original dis1 dataset to a tsibble object, we use the as_tsibble() function and specify the index, i.e. the variable specifying the time unit of interest (year and week variables in our case). We are not using key variables for now. Variables that are not used for index or key purposes, such as the cases variables, are considered as measured variables.\nTo define the index, we can make use of the tsibble built-in functions such as make_yearweek (or yearmonth, yearquarter, etc., see documentation)\n\n\nShow the code\ndis1_ts &lt;- dis1 %&gt;%\n  mutate(date_index = make_yearweek(year = year, week = week)) %&gt;%\n  as_tsibble(index = date_index)\n\nhead(dis1_ts, 5)\n\n\n# A tsibble: 5 x 4 [1W]\n   year  week cases date_index\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;week&gt;\n1  1981     1     1   1981 W01\n2  1981     2     3   1981 W02\n3  1981     3    NA   1981 W03\n4  1981     4     4   1981 W04\n5  1981     5     3   1981 W05\n\n\nYou can see how the number of dis1 cases is distributed in time using the ggplot package.\n\n\nShow the code\nggplot(data = dis1_ts) +\n    geom_line(mapping = aes(x = date_index, y = cases)) +\n    scale_x_yearweek(date_labels = \"%Y\") +\n    labs(x = \"Date\", y = \"Number of Cases\", title = \"Disease 1 data\") +\n    theme_bw() + \n        theme(\n            plot.title = element_text(face = \"bold\", \n                                      size = 12),\n            legend.background = element_rect(fill = \"white\", \n                                             size = 4, \n                                             colour = \"white\"),\n            # legend.justification = c(0, 1),\n            legend.position = \"bottom\",\n            panel.grid.minor = element_blank()\n        )\n\n\n\n\n\n\n\n\n\nOne tip: if you design ggplot themes through more complex theme() customizations, you can save it in an object and later use it in your plots. This way, you only need to write the code once which helps keeping the consistency across your entire plots\n\n\nShow the code\n# We can save theme modifications into a single object and then use it in new plots\ntsa_theme &lt;- theme_bw() + \n        theme(\n            plot.title = element_text(face = \"bold\", \n                                      size = 12),\n            legend.background = element_rect(fill = \"white\", \n                                             size = 4, \n                                             colour = \"white\"),\n            # legend.justification = c(0, 1),\n            legend.position = \"bottom\",\n            panel.grid.minor = element_blank()\n        )\n\n\nNote from the plot that there are missing values in the data.\nIf data points are collected at regular time interval, we can correct missing data by providing a default value or interpolating missing values from other data. The tsibble package has the fill_gaps function, which fills missing values with a pre-specified default value. It is quite common to replaces NAs with its previous observation for each time point in a time series analysis, which is easily done using the fill function from tidyr package.\nA quick overview of implicit missing values with tsibble is available on vignette(\"implicit-na\").\n\n\n\n\n\n\nNote\n\n\n\nFor time series data visualization, there are specific packages and functions you can use. As the focus of this training is on understanding principles of time series analysis rather than visualisation of time series, we will mainly use the ggplot functions for the remaining exercises (also so you get to practise more complex plots)\n\n\n\n\nPumala data\nLet’s now work with the Pumala data\nHere you have one variable for the year, one variable for the month and one variable with the complete date in days, but in a text (chr class) format.\nWith the complete date, you can generate ISO weeks, but first you need the date variable to be converted from text to something R recognises as a date. The lubridate package has a number of convenient functions for converting text strings to dates.\n\n\nShow the code\ndis3 &lt;- dis3 %&gt;%\n  mutate(my_date = dmy(date_str))\n\nstr(dis3$my_date)\n\n\n Date[1:3844], format: \"1995-01-02\" \"1995-01-02\" \"1995-01-03\" \"1995-01-03\" \"1995-01-04\" ...\n\n\nAs a complementary note, check the help for strftime to learn about different date formats in R (?strftime).\nTo convert to ISO week, we can use the ISOweek function from the ISOweek package, which creates a new variable representing the ISO week. We could then also create a new variable representing the first Monday of each ISO week. Although the popular lubridate package has an isoweek function (there is also a similar function in the surveillance package), we use the ISOweek package here as it has the ISOweek2date function. If epidemiological weeks are required, use the EpiWeek package.\nThe paste command concatenates text.\nHere we are adding \"-01\" onto the end of the ISO week variable (which is formatted something like \"1995-W01\"), to indicate that we want the first day of that week, and then supplying that to the ISOweek2date function, which converts that to a date.\nHave a look at the new variables that have been created. date_isowk is the ISO week variable, in string format, and isodate is the Monday of each ISO week. Note that the years 1998 and 2004 each have an ISO week 53.\nYou have several observations in the same week since you have data from different days and one value corresponds to one case. Use the count function to aggregate the data.\n\n\nShow the code\ndis3_v2 &lt;- dis3 %&gt;%\n    count(date_isowk)\n\nhead(dis3_v2)\n\n\n  date_isowk  n\n1   1995-W01 13\n2   1995-W02 15\n3   1995-W03  6\n4   1995-W04  5\n5   1995-W05 11\n6   1995-W06 10\n\n\n\n\nShow the code\ndis3_ts &lt;- dis3_v2 %&gt;%\n    mutate(date_index = yearweek(x = date_isowk, week_start = 1L)) %&gt;%\n    as_tsibble(index = date_index)\n\n\nggplot(data = dis3_ts) +\n    geom_line(mapping = aes(x = date_index, y = n)) +\n    scale_x_yearweek(date_labels = \"%Y\") +\n    labs(x = \"Date\", \n         y = \"Number of Cases\", \n         title = \"Disease 1 data\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nAggregating cases by month is another possibility.\n\n\nShow the code\ndis3_agg &lt;- dis3 %&gt;%\n    count(year, month)\n\ndis3_ts_v2 &lt;- dis3_agg %&gt;%\n    mutate(date_index = make_yearmonth(year = year, month = month)) %&gt;%\n    as_tsibble(index = date_index)\n\nggplot(data = dis3_ts_v2) +\n    geom_line(mapping = aes(x = date_index, y = n)) +\n    scale_x_yearweek(date_labels = \"%Y\") +\n    labs(x = \"Date\", \n         y = \"Number of Cases\", \n         title = \"Disease 1 data\") +\n    tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "scripts/03-practical.html#solution-3-1-1",
    "href": "scripts/03-practical.html#solution-3-1-1",
    "title": "Practical Session 3: Managing date formats and plotting",
    "section": "3.2 Optional task",
    "text": "3.2 Optional task\nImport your data to R from the dis2 excel file.\n\n\nShow the code\ndis2 &lt;- import(here(\"data\", \"tsa_practice.xlsx\"), which = \"dis2\")\nView(dis2)\n\n\nInspect the data.\nYou have separate columns containing the counts. To plot this data, you need to first reshape your dataset by converting it from the current wide format to a long format.\nThe function pivot_longer can perform such transformation.\n\n\nShow the code\ndis2l &lt;- dis2 %&gt;%\n  pivot_longer(\n      cols = -year, \n      names_to = \"month\", \n      values_to = \"case\"\n  ) %&gt;%\n  mutate(month = as_factor(month)) %&gt;%  # as_factor sets levels in the order they appear\n  arrange(year, month)\n\nstr(dis2l)\n\n\ntibble [528 × 3] (S3: tbl_df/tbl/data.frame)\n $ year : num [1:528] 1928 1928 1928 1928 1928 ...\n $ month: Factor w/ 12 levels \"Jan\",\"Feb\",\"Mar\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ case : num [1:528] 609 1516 4952 7466 11155 ...\n\n\nShow the code\nhead(dis2l)\n\n\n# A tibble: 6 × 3\n   year month  case\n  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n1  1928 Jan     609\n2  1928 Feb    1516\n3  1928 Mar    4952\n4  1928 Apr    7466\n5  1928 May   11155\n6  1928 Jun    7002\n\n\nShow the code\ndis2l_agg &lt;- dis2l %&gt;%\n    mutate(date_index = make_yearmonth(year = year, month = month)) %&gt;%\n    as_tsibble(index = date_index)\n\nhead(dis2l_agg)\n\n\n# A tsibble: 6 x 4 [1M]\n   year month  case date_index\n  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;      &lt;mth&gt;\n1  1928 Jan     609  1928 ene.\n2  1928 Feb    1516  1928 feb.\n3  1928 Mar    4952  1928 mar.\n4  1928 Apr    7466  1928 abr.\n5  1928 May   11155  1928 may.\n6  1928 Jun    7002  1928 jun.\n\n\nShow the code\nggplot(data = dis2l_agg) +\n    geom_line(mapping = aes(x = date_index, y = case)) +\n    scale_x_yearweek(date_labels = \"%Y\") +\n    labs(x = \"Date\", \n         y = \"Number of Cases\", \n         title = \"Disease 3 data\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\ndis1 corresponds to salmonellosis cases, dis2 to measles cases in New York.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 3: Managing date formats and plotting"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis (TSA) Module for the ECDC EPIET & EUPHEM Program",
    "section": "",
    "text": "Welcome\nWelcome to the Time Series Analysis (TSA) module webpage of the ECDC EPIET & EUPHEM Program.\nHere you will find the Practical Sessions 3-7, 9 and 10 of the TSA module 2025, including the instructions, guide and solution of the proposed exercises. Sessions 8 and 11 will be provided as stand-alone exercises",
    "crumbs": [
      "HOMEPAGE"
    ]
  },
  {
    "objectID": "index.html#install-r-rtools-and-rstudio",
    "href": "index.html#install-r-rtools-and-rstudio",
    "title": "Time Series Analysis (TSA) Module for the ECDC EPIET & EUPHEM Program",
    "section": "Install R, Rtools and Rstudio",
    "text": "Install R, Rtools and Rstudio\nParticipants need to have R, RTools, and RStudio installed in their computer. Preferably, the installation of these applications should be done in the stated order: R &gt;&gt; RTools &gt;&gt; RStudio. If you don’t have these applications installed yet, please do so following the instructions below:\n\nR should be installed in the participants’ computer. You can download it from here, on the section “Download and Install R”. Select the adequate version to your computer’s operating system.\n\nLatest version available is 4.5.2 (31st Oct 2025). You don’t need the latest version installed. However, it’s highly recommended to stay updated on at least the ‘major version’ (4.x.x) and not too many ‘minor versions’ (x.5.x) away, as packages tend to update to each of them. A warning message might show when using p_load if your R version is older than a package’s last update; however, it will work fine.\nMultiple versions of R can be installed in one’s computer. R usually can be installed without administrative rights. If you have more than one R version installed on your computer, please make sure that you select the specific version 4.2.3. when running RStudio (Top Menu: Settings &gt; Global Options &gt; Left Panel &gt; R)\n\nRTools at the same R version you are using should also be installed in the participants’ computer. You can download the necessary files by clicking here and selecting the corresponding operating system in your computer.\n\nFor MacOS, RTools installation comprises the minimal installation of clang and gfortran as listed in the hyperlinked website. Please note that preferably only one version of RTools version 4.2 or below should be installed in one’s computer. RTools version 4.3 can be installed simultaneously with RTools version 4.2. or below in one’s computer.\n\nRStudio can be used with previous versions without creating conflict with packages or R versions safely. Try to keep updated if possible.",
    "crumbs": [
      "HOMEPAGE"
    ]
  },
  {
    "objectID": "index.html#libraries",
    "href": "index.html#libraries",
    "title": "Time Series Analysis (TSA) Module for the ECDC EPIET & EUPHEM Program",
    "section": "Libraries",
    "text": "Libraries\nThe following R packages have to be installed. Open RStudio, copy the highlighted code below into the R Console pane and press enter. This process might take several minutes.\n\n\nShow the code\n# Install pacman if not installed already, and activate it\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n# Install, update and activate libraries\npacman::p_load(\n  here, \n  rio, \n  skimr,\n  tsibble,\n  ISOweek,\n  slider,\n  pander,\n  season,\n  lmtest,\n  TSA,\n  ciTools,\n  gtsummary,\n  patchwork,\n  AER,\n  MASS,\n  sjPlot,\n  tidyverse\n)\n\n\n## packages to be installed for outbreak detection / loaded by pacman\npacman::p_load(devtools)\ndevtools::install_github(\"United4Surveillance/signal-detection-tool\")\n\n\n## packages to be installed for multilevel / loaded by pacman\npacman::p_load(devtools)\ndevtools::install_github(\"goodekat/redres\")\n\n\nFrequent problems you may encounter during the SignalDetectionTool installation:\n\nPlease start a new R session before the installation by going in Rstudio to Session \n\nSolution: RStudio menu bar &gt; Session &gt; New Session\n\nIn case you get an error message that some package with version x.x.x. is already loaded in the namespace, but a higher version is requested, you should try:\n\nMake a new R session\nIf 1) did not work, then manually install_packages(package_with_problem).\nInstall the SignalDetectionTool again\n\n\nIn case you have troubles accessing Github check below\nInstallation Instructions for the SignalDetectionTool in case of problems with GitHub access\n\nSave the file signal-detection-tool-0.8.0.tar.gz on your computer\nOpen RStudio and run install.packages(\"remotes\")\nInstall all dependencies for the SignalDetectionTool by running and filling in your path to the stored .tar.gz with the command remotes::install_deps(\"your_path_to_signal-detection-tool-0.8.0.tar.gz\"\nInstall the SignalDetectionTool by running install.packages(\"your_path_to_signal-detection-tool-0.8.0.tar.gz\", repos = NULL, type = \"source\")",
    "crumbs": [
      "HOMEPAGE"
    ]
  },
  {
    "objectID": "scripts/09-practical.html#session-inject",
    "href": "scripts/09-practical.html#session-inject",
    "title": "Practical Session 9: The relation between two time series",
    "section": "",
    "text": "Expected learning outcomes\nBy the end of this session, participants should be able to:\n\nAssess and interpret associations with external variables\nIdentify and interpret effect modification between external variables and the outcome variable in surveillance data\n\n\n\nSource code",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 9: The relation between two time series"
    ]
  },
  {
    "objectID": "scripts/04-practical.html",
    "href": "scripts/04-practical.html",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "",
    "text": "Mortality Surveillance data in Spain\nThe number of deaths in a country can be used as a surveillance indicator. An example of such an application is EuroMOMO, which monitors number of fatalities in European countries on a weekly basis (https://www.euromomo.eu/). The weekly time series on the number of deathls are routinely used to follow seasonal infections, for example influenza and covid 19 during the winter, but also heatwaves in the summer.\nIn case studies 4 to 7, we will analyse the number of deaths retrieved from the Spanish national statistical institute (INE) (https://ine.es/). Each case study will build up from the analysis conducted in the previous case study. The data contains the number of fatalities per week for the entire country, from 2010 to 2019, and includes variables on year, week, total number of deaths (cases), and total population; the same data is also included for men and women.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#learn-4",
    "href": "scripts/04-practical.html#learn-4",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Expected learning outcomes",
    "text": "Expected learning outcomes\nBy the end of the session, participants should be able to:\n\nDescribe, test and fit a trend in surveillance data (simple smoothing and regression);\nAssess and interpret the significance of trend in surveillance data.\n\nSave any changes to a new dataset named `mortagg.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-1",
    "href": "scripts/04-practical.html#task-4-1",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.1",
    "text": "Task 4.1\nAssess visually how the total registered number of deaths has been behaving in the years with available data.\nDiscuss your results with your peers.\n\nMoving average and direct statistical modelling of trends\nMoving averages are simple methods to visualise the general trend of a series after removing some of the random day-to-day variation by smoothing the data. This allows you to browse your data for periodicity and observe the general trend. In other words, smoothing the data may remove “noise” from your time series and can facilitate visual interpretation.\nMoving averages model a time series by calculating the numerical mean of the values adjacent to it. It is calculated for each observation, moving along the time axis: e.g. at each time \\(t\\) and for a window of 5 time units, one way of calculating the moving average is by using the observations at \\(t-2\\), \\(t-1\\), \\(t\\), \\(t+1\\) and \\(t+2\\).",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-1",
    "href": "scripts/04-practical.html#solution-4-1",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.1",
    "text": "Help for Task 4.1\nLoad the mortagg.Rdata dataset.\n\n\nShow the code\nload(here(\"data\", \"mortagg2.Rdata\"))\n\n\nInspect the data.\n\n\nShow the code\nstr(mortagg)\n\n\n'data.frame':   521 obs. of  8 variables:\n $ cases_m: num  4207 4409 4229 4252 4196 ...\n $ cases_f: num  4019 4264 4122 3837 3976 ...\n $ cases  : num  8226 8673 8351 8089 8172 ...\n $ year   : num  2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ...\n $ week   : num  1 2 3 4 5 6 7 8 9 10 ...\n $ pop    : num  46486621 46486621 46486621 46486621 46486621 ...\n $ pop_f  : num  23504349 23504349 23504349 23504349 23504349 ...\n $ pop_m  : num  2.3e+07 2.3e+07 2.3e+07 2.3e+07 2.3e+07 ...\n\n\nShow the code\nsummary(mortagg)\n\n\n    cases_m        cases_f         cases            year           week      \n Min.   :3276   Min.   :2941   Min.   : 6316   Min.   :2010   Min.   : 1.00  \n 1st Qu.:3648   1st Qu.:3464   1st Qu.: 7106   1st Qu.:2012   1st Qu.:14.00  \n Median :3839   Median :3684   Median : 7510   Median :2015   Median :27.00  \n Mean   :3969   Mean   :3819   Mean   : 7788   Mean   :2015   Mean   :26.55  \n 3rd Qu.:4194   3rd Qu.:4047   3rd Qu.: 8217   3rd Qu.:2017   3rd Qu.:40.00  \n Max.   :5597   Max.   :5877   Max.   :11471   Max.   :2019   Max.   :53.00  \n      pop               pop_f              pop_m         \n Min.   :46418884   Min.   :23504349   Min.   :22806445  \n 1st Qu.:46486621   1st Qu.:23612439   1st Qu.:22831107  \n Median :46497393   Median :23621034   Median :22889600  \n Mean   :46608292   Mean   :23669979   Mean   :22938313  \n 3rd Qu.:46712650   3rd Qu.:23719207   3rd Qu.:23016783  \n Max.   :46918951   Max.   :23902168   Max.   :23099009  \n\n\nShow the code\nview(mortagg)\n\n\n\n\nShow the code\nskimr::skim(mortagg) \n\n\n\nData summary\n\n\nName\nmortagg\n\n\nNumber of rows\n521\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncases_m\n0\n1\n3969.15\n454.44\n3276\n3648\n3839\n4194\n5597\n▇▇▃▁▁\n\n\ncases_f\n0\n1\n3818.53\n544.44\n2941\n3464\n3684\n4047\n5877\n▆▇▃▁▁\n\n\ncases\n0\n1\n7787.68\n992.14\n6316\n7106\n7510\n8217\n11471\n▇▇▃▁▁\n\n\nyear\n0\n1\n2014.50\n2.87\n2010\n2012\n2015\n2017\n2019\n▇▇▇▇▇\n\n\nweek\n0\n1\n26.55\n15.05\n1\n14\n27\n40\n53\n▇▇▇▇▇\n\n\npop\n0\n1\n46608291.50\n163065.99\n46418884\n46486621\n46497393\n46712650\n46918951\n▇▁▅▂▂\n\n\npop_f\n0\n1\n23669978.80\n102454.81\n23504349\n23612439\n23621034\n23719207\n23902168\n▂▇▆▂▂\n\n\npop_m\n0\n1\n22938312.61\n100399.88\n22806445\n22831107\n22889600\n23016783\n23099009\n▇▅▁▇▅\n\n\n\n\n\nCreate a time series object with tsibble, using the total case counts and plot the time series.\n\n\nShow the code\nmortz &lt;-\n  mortagg %&gt;%\n  mutate(date_index = make_yearweek(year = year, week = week)) %&gt;%\n  as_tsibble(index = date_index)\n\n\nggplot(data = mortz) +\n  geom_line(mapping = aes(x = date_index, y = cases)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  labs(x = \"Year Week\", y = \"Number of Deaths\", title = \"Spain: number of deaths per week\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n## Adjusting y-axis scale\nggplot(data = mortz) +\n  geom_line(mapping = aes(x = date_index, y = cases)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Date\", y = \"Number of Deaths\", title = \"Spain: number of deaths per week\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nCreate a similar plot for men and women.\n\n\nShow the code\n## Men and Women in wide format\n\ncolors &lt;- c(\"cases_m\"=\"green\", \"cases_f\"=\"black\")\n\nggplot(data = mortz, aes(x = date_index)) +\n  geom_line(mapping = aes(y = cases_m, colour = \"cases_m\"), lwd=1) +\n  geom_line(mapping = aes(y = cases_f, colour = \"cases_f\"), lwd=1) + \n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Week\", y = \"Number of Deaths\", title = \"Spain: number of deaths per week and by sex\", \n         colour = \"Legend\") +\n  scale_color_manual(values = colors, name=\"\", labels = c(\"Men\", \"Women\")) +\n  tsa_theme",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-2",
    "href": "scripts/04-practical.html#task-4-2",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.2",
    "text": "Task 4.2\nAssess visually the existence of a long-term trend and periodicity in the mortaagg dataset by using smoothing techniques.\nConsider: where would you centre the smoothing window and why? What is the effect of using different smoothing window centring options? Test!\nOf all the methods applied here (moving average with different windows, loess with different spans), which one do you think is the best for eliminating seasonality? Where would you centre the smoothing window and why? What would happen if you used an even greater window? Test!",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-2",
    "href": "scripts/04-practical.html#solution-4-2",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.2",
    "text": "Help for Task 4.2\nAccording to the slider package description, slider is a package for rolling windows analysis. It means that a given function is repeatedly applied to different “windows” of your data as you step through it. Typical examples of applications of rolling window functions include moving averages or cumulative sums. An introduction vignette for slider can be found by running the following command: vignette(\"slider\")\nFor each record, create the following various types of moving average.\n\nMA5a: the 5-week moving average, centered on cases.\nMA5b: the 5-week moving average of cases and the 4 previous weeks.\nMA5c: the 5-week moving average of the 5 previous weeks.\n\nCompare results.\n\n\nShow the code\nmortzma &lt;-\n  mortz %&gt;%\n  mutate(\n    MA5a = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 2,\n      .after = 2,\n      .complete = TRUE\n    ),\n    MA5b = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 4,\n      .complete = TRUE\n    ),\n    MA5c = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = function(x) mean(x[-6], na.rm = TRUE),\n      .before = 5,\n      .complete = TRUE\n    )\n  )\n\n# view first 10 lines of data\nhead(mortzma, 10)\n\n\n# A tsibble: 10 x 12 [1W]\n   cases_m cases_f cases  year  week    pop  pop_f  pop_m date_index  MA5a  MA5b\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    4207    4019  8226  2010     1 4.65e7 2.35e7 2.30e7   2010 W01   NA    NA \n 2    4409    4264  8673  2010     2 4.65e7 2.35e7 2.30e7   2010 W02   NA    NA \n 3    4229    4122  8351  2010     3 4.65e7 2.35e7 2.30e7   2010 W03 8302.   NA \n 4    4252    3837  8089  2010     4 4.65e7 2.35e7 2.30e7   2010 W04 8253.   NA \n 5    4196    3976  8172  2010     5 4.65e7 2.35e7 2.30e7   2010 W05 8183  8302.\n 6    4127    3854  7981  2010     6 4.65e7 2.35e7 2.30e7   2010 W06 8131. 8253.\n 7    4294    4028  8322  2010     7 4.65e7 2.35e7 2.30e7   2010 W07 8015  8183 \n 8    4121    3968  8089  2010     8 4.65e7 2.35e7 2.30e7   2010 W08 7887. 8131.\n 9    3824    3687  7511  2010     9 4.65e7 2.35e7 2.30e7   2010 W09 7853. 8015 \n10    3921    3612  7533  2010    10 4.65e7 2.35e7 2.30e7   2010 W10 7710. 7887.\n# ℹ 1 more variable: MA5c &lt;dbl&gt;\n\n\nThe slide_index_dbl is a slider package function designed to run a pre-specified function on a rolling window of a numeric (_dbl) variable, ordered by an index time (date or date-time) variable. A full description of this function can be found by running the following command: ?slide_index_dbl\nThe .x argument provides the vector with the numbers that will be used for computation.\nThe .i argument defines the vector with the time variable that will be used for ordering the data.\nThe .f argument indicates the function that will be used to perform the intended computations. In this case, an arithmetic mean computation is carried out by using the mean function. The na.rm = TRUE is a varying argument included as a ... argument of slide_index_dbl. (Run the expression args(\"slide_index_dbl\") and take notice on the position/sequence of this function’s arguments).\nThe .before argument defines the number of observations before the central time point of a given time window to use in the rolling window computation. In the example above for a 5 time points centered moving average, in MA5a, 2 observations are used before the central point, and 2 observations are used after it, using the .after argument.\nThe complete argument indicates where the computation should be carried in rolling windows that have complete observations.\nWe applied a more complicated function to compute MA5c, taking a backwards moving window of six values but omitting the latest value (current time point) from the calculation of the average.\n\n\nShow the code\n## wide format dataset ---\n\ncolors &lt;- c(\"cases\"=\"black\", \"MA5a\"=\"red\", \"MA5b\"=\"blue\", \"MA5c\"=\"brown\")\n\nggplot(data = mortzma, mapping = aes(x = date_index)) +\n  geom_line(mapping = aes(y = cases, colour = \"cases\")) +\n  geom_line(mapping = aes(y = MA5a, colour = \"MA5a\")) +\n  geom_line(mapping = aes(y = MA5b, colour = \"MA5b\")) +\n  geom_line(mapping = aes(y = MA5c, colour = \"MA5c\")) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Week\", \n       y = \"Number of deaths\", \n       title = \"Spain: number of deaths - moving averages\",\n       colour=\"Legend\") +\n  scale_color_manual(values = colors, name=\"\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nWe observe that the calculation is similar across these various methods, but is not aligned to the series in the same way. MA5a is centered in the middle of the period used to calculate the mean. MA5b is placed at the end of the period. MA5c is placed one step forward (smoothing functions can be used for forecasting the following point). The “models” provided are similar for a 5-week window, but the lag is different.\nMoving average is only one way of smoothing. Other ways of smoothing the data to get a general idea of the trend include, for example, LOESS (locally estimated scatterplot smoothing) smoothing, where the contribution of surrounding observations is weighted, i.e. it is not the arithmetical mean for each set (window) of observations.\nWhy do you think we have chosen these windows for the moving average?\nTo better observe the general trend, we need to find the length of the moving average that will erase the seasonal component. Various lengths can be tried; here we have used 25, 51 and 103.\n\n\nShow the code\nmortzma2 &lt;-\n  mortz %&gt;%\n  mutate(\n    MA25 = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 24,\n      .complete = TRUE\n    ),\n    MA51 = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 50,\n      .complete = TRUE\n    ),\n    MA103 = slide_index_dbl(\n      .x = cases,\n      .i = date_index,\n      .f = mean,\n      na.rm = TRUE,\n      .before = 102,\n      .complete = TRUE\n    )\n  )\n\n## Visually inspect data\nslice(mortzma2, 20:30)\n\n\n# A tsibble: 11 x 12 [1W]\n   cases_m cases_f cases  year  week    pop  pop_f  pop_m date_index  MA25  MA51\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    3867    3436  7303  2010    20 4.65e7 2.35e7 2.30e7   2010 W20   NA     NA\n 2    3758    3446  7204  2010    21 4.65e7 2.35e7 2.30e7   2010 W21   NA     NA\n 3    3605    3595  7200  2010    22 4.65e7 2.35e7 2.30e7   2010 W22   NA     NA\n 4    3440    3076  6516  2010    23 4.65e7 2.35e7 2.30e7   2010 W23   NA     NA\n 5    3407    3124  6531  2010    24 4.65e7 2.35e7 2.30e7   2010 W24   NA     NA\n 6    3632    3201  6833  2010    25 4.65e7 2.35e7 2.30e7   2010 W25 7531.    NA\n 7    3690    3567  7257  2010    26 4.65e7 2.35e7 2.30e7   2010 W26 7492.    NA\n 8    3764    3625  7389  2010    27 4.65e7 2.35e7 2.30e7   2010 W27 7441.    NA\n 9    3697    3638  7335  2010    28 4.65e7 2.35e7 2.30e7   2010 W28 7400.    NA\n10    3570    3349  6919  2010    29 4.65e7 2.35e7 2.30e7   2010 W29 7353.    NA\n11    3476    3209  6685  2010    30 4.65e7 2.35e7 2.30e7   2010 W30 7294.    NA\n# ℹ 1 more variable: MA103 &lt;dbl&gt;\n\n\nShow the code\nslice(mortzma2, 45:55)\n\n\n# A tsibble: 11 x 12 [1W]\n   cases_m cases_f cases  year  week    pop  pop_f  pop_m date_index  MA25  MA51\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    3771    3563  7334  2010    45 4.65e7 2.35e7 2.30e7   2010 W45 6891.   NA \n 2    3817    3561  7378  2010    46 4.65e7 2.35e7 2.30e7   2010 W46 6898.   NA \n 3    4049    3516  7565  2010    47 4.65e7 2.35e7 2.30e7   2010 W47 6912.   NA \n 4    4001    3794  7795  2010    48 4.65e7 2.35e7 2.30e7   2010 W48 6963.   NA \n 5    4058    3754  7812  2010    49 4.65e7 2.35e7 2.30e7   2010 W49 7015.   NA \n 6    4034    3668  7702  2010    50 4.65e7 2.35e7 2.30e7   2010 W50 7049.   NA \n 7    4061    3892  7953  2010    51 4.65e7 2.35e7 2.30e7   2010 W51 7077. 7303.\n 8    4484    4056  8540  2010    52 4.65e7 2.35e7 2.30e7   2010 W52 7123. 7309.\n 9    4593    4315  8908  2011     1 4.67e7 2.36e7 2.30e7   2011 W01 7186. 7314.\n10    4276    4194  8470  2011     2 4.67e7 2.36e7 2.30e7   2011 W02 7248. 7316.\n11    4271    4175  8446  2011     3 4.67e7 2.36e7 2.30e7   2011 W03 7319. 7323.\n# ℹ 1 more variable: MA103 &lt;dbl&gt;\n\n\nShow the code\nslice(mortzma2, 95:105)\n\n\n# A tsibble: 11 x 12 [1W]\n   cases_m cases_f cases  year  week    pop  pop_f  pop_m date_index  MA25  MA51\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    3824    3513  7337  2011    43 4.67e7 2.36e7 2.30e7   2011 W43 6919. 7420.\n 2    3818    3643  7461  2011    44 4.67e7 2.36e7 2.30e7   2011 W44 6936. 7423.\n 3    3737    3698  7435  2011    45 4.67e7 2.36e7 2.30e7   2011 W45 6958. 7424.\n 4    3789    3644  7433  2011    46 4.67e7 2.36e7 2.30e7   2011 W46 6965. 7421.\n 5    3744    3602  7346  2011    47 4.67e7 2.36e7 2.30e7   2011 W47 6987. 7412.\n 6    3971    3708  7679  2011    48 4.67e7 2.36e7 2.30e7   2011 W48 7025. 7410.\n 7    4025    3824  7849  2011    49 4.67e7 2.36e7 2.30e7   2011 W49 7063. 7413.\n 8    4148    3882  8030  2011    50 4.67e7 2.36e7 2.30e7   2011 W50 7102. 7414.\n 9    4354    4054  8408  2011    51 4.67e7 2.36e7 2.30e7   2011 W51 7141. 7411.\n10    4476    4277  8753  2011    52 4.67e7 2.36e7 2.30e7   2011 W52 7214. 7408.\n11    4425    4304  8729  2012     1 4.68e7 2.37e7 2.31e7   2012 W01 7289. 7413.\n# ℹ 1 more variable: MA103 &lt;dbl&gt;\n\n\nYou can use View to examine the first rows of mortzma2.\n\n\nShow the code\nView(mortzma2)\n\n\nPlot the times series with the different moving averages.\n\n\nShow the code\n## wide format dataset ---\n\ncolors &lt;- c(\"cases\"=\"black\", \"MA25\"=\"red\", \"MA51\"=\"blue\", \"MA103\"=\"orange\")\n\nggplot(data = mortzma2, mapping = aes(x = date_index)) +\n  geom_line(mapping = aes(y = cases, colour = \"cases\"), linewidth=1) +\n  geom_line(mapping = aes(y = MA25, colour = \"MA25\"), linewidth=1.2) +\n  geom_line(mapping = aes(y = MA51, colour = \"MA51\"), linewidth=1.2) +\n  geom_line(mapping = aes(y = MA103, colour = \"MA103\"), linewidth=1.2) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Year-Week\", \n       y = \"Number of deaths\", \n       title = \"Spain: number of deaths - moving averages with 26, 51 and 103-week windows\",\n       colour=\"Legend\") +\n  scale_color_manual(values = colors, name=\"\", breaks = names(colors)[c(1,2,3,4)]) +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nMoving average is only one way of smoothing. Other ways of smoothing the data to get a general idea of the trend include, for example, LOESS (locally estimated scatterplot smoothing) smoothing, where the contribution of surrounding observations is weighted, i.e. it is not the arithmetical mean for each set (window) of observations.\nThe geom_smooth provides a smoothing curve using a LOESS method.\n\n\nShow the code\nggplot(mortz, mapping = aes(x = date_index, y = cases)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = TRUE) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Date\", \n       y = \"Number of Deaths\", \n       title = \"Loess smoothing\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nWe can change the degree of smoothing by adding a span option. The span controls the amount of smoothing for the default loess smoother; smaller numbers produce wigglier lines, and larger numbers produce smoother lines.\n\n\nShow the code\nggplot(data = mortz, mapping = aes(x = date_index, y = cases)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = TRUE, span = 0.1) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Date\", y = \"Number of Deaths\", title = \"Loess smoothing (span = 0.1)\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nIn ggplot(), we can also use stat_smooth, which provides a smoothing curve using a LOESS method, and were wee can change the degree of smoothing by adding a span option. The span controls the amount of smoothing for the default loess smoother; smaller numbers produce wigglier lines, and larger numbers produce smoother lines.\n\n\nShow the code\nggplot(data = mortz, mapping = aes(x = date_index, y = cases)) +\n  geom_point(alpha = 0.5) +\n  stat_smooth(se = TRUE, span=0.1) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = \"Year-Week\", y = \"Number of deaths\", \n    title = \"Spain: number of deaths - loess smoothing (span = 0.1)\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nTest different values of span. Comment on the lines provided by the different smoothing windows. Which one do you think is the best for eliminating seasonality? What would happen if you used an even greater window?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-3",
    "href": "scripts/04-practical.html#task-4-3",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.3",
    "text": "Task 4.3\nAssess statistically whether the registered number of deaths has been stable in the years available in your dataset.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-3",
    "href": "scripts/04-practical.html#solution-4-3",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.3",
    "text": "Help for Task 4.3\nUsing regression methods against the time variable is a simple and familiar way to look at trends and test the slope. The kind of regression and interpretation you will use depends on the nature of your dependent variable (outcome), in this case, the number (count) of registered deaths. Time will be represented by a variable that simply enumerates the observations; in other words, time is a variable with values from 1 (the first observation in time; here week 1, 2010) to n (the last observation in time; here week 52, 2019).\n\n\nShow the code\nmortz &lt;-\n  mortz %&gt;%\n  mutate(index = seq.int(from = 1, to = nrow(.)))\n\nmort_poissontrend &lt;- glm(formula = cases ~ index,\n                          family = \"poisson\",\n                          data = mortz\n)\n\n\nCheck model results.\n\n\nShow the code\nsummary(mort_poissontrend)\n\n\n\nCall:\nglm(formula = cases ~ index, family = \"poisson\", data = mortz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 8.910e+00  1.007e-03 8850.25   &lt;2e-16 ***\nindex       1.895e-04  3.302e-06   57.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 59215  on 519  degrees of freedom\nAIC: 64841\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe glm function fits a linear regression model to the log of the weekly number of deaths. You can use the names function to see the various components of the output of the glm function, as above. As mentioned in the previous session, the str function is also often useful for looking at the “structure” of the output of an R function. Summary provides a quick function for checking the results.\nVariables in R models are specified using notation along the lines of response_variable ~ explanatory_variable_1 + explanatory_variable_2 etc. Check the material from the MVA module. A more detailed explanation of how to build model formulas in R can also be found in the Details section of the help page of the formula function: ?stats::formula.\nsummary, when given the results of fitting a regression model, will provide a summary of the fitted parameters (coefficients); in this case the parameter of index is the trend.\nIdentify and interpret the intercept and the trend. In a Poisson model on the log(y), the coefficient needs to be exponentiated in order to interpret it for the output y (that is, the original y without the log).\nPlot the fitted values against the observed ones.\n\n\nShow the code\ncolors &lt;- c(\"observed\"=\"black\", \"fitted\"=\"blue\")\n\nggplot(data = mortz, mapping = aes(x = date_index)) +\n  geom_point(mapping = aes(y = cases, colour=\"observed\"), alpha = 0.5) +\n  geom_line(mapping = aes(y = fitted(mort_poissontrend), colour = \"fitted\"), linewidth=1.1) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", date_breaks = \"1 year\") +\n  labs(x = \"Year-Week\", y = \"Number of deaths\", title = \"Spain: number of deaths per week with fitted trend\") +\n  scale_color_manual(values = colors, name=\"\") +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nThe trend as incidence rate ratio (IRR) provides the average relative change in the number of deaths from one week to the next.\n\n\nShow the code\n# Trend in the original scale of the outcome cases (deaths) wit 95% confidence intervals. \nIRR_trend &lt;- exp(cbind(Estimate=mort_poissontrend$coef, confint(mort_poissontrend)))\n\n(IRR_trend['index', ] - 1) *100\n\n\n  Estimate      2.5 %     97.5 % \n0.01894927 0.01830203 0.01959652 \n\n\nThe result above indicates that from one week to another, the number of deaths increases in average 0,019% per week, with a 95% confidence interval of [0,018%; 0,020%].\nAnother way in R to obtain the incidence rate ratio (IRR) is:\n{r: task-4-3-IRR-gtsummary} mort_poissontrend %&gt;%    gtsummary::tbl_regression(     exponentiate = T,     estimate_fun = ~style_number(.x, digits = 4)   )\nThis code creates a regression table using the tbl_regression() function from the gtsummary package. It has two arguments: - exponentiate = T (or TRUE) This tells R to exponentiate the coefficients. - estimate_fun = ~style_number(.x, digits = 4) This controls how the numbers in your table are formatted. .x represents each coefficient value, and style_number() formats it to 4 decimal places.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-4",
    "href": "scripts/04-practical.html#task-4-4",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.4",
    "text": "Task 4.4\nHow could you take the changing population size of Spain into account in your analyses?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#solution-4-4",
    "href": "scripts/04-practical.html#solution-4-4",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Help for Task 4.4",
    "text": "Help for Task 4.4\nTo take the population into account, we need to include the population in the model. This is done by adding a term called offset(). The parameter beta for the offset, in this case for the population, will be forced to be 1; in other words, the offset will not affect the outcome. The advantage is that we can interpret the IRR of time in terms of mortality rate = cases/population.\n\n\nShow the code\nmort_poissontrend_pop &lt;- glm(cases ~  index + offset(log(pop)),\n                          data = mortz,\n                          family = \"poisson\"\n                        )\n\nsummary(mort_poissontrend_pop)\n\n\n\nCall:\nglm(formula = cases ~ index + offset(log(pop)), family = \"poisson\", \n    data = mortz)\n\nCoefficients:\n              Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept) -8.746e+00  1.006e-03 -8691.66   &lt;2e-16 ***\nindex        1.863e-04  3.299e-06    56.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62588  on 520  degrees of freedom\nResidual deviance: 59399  on 519  degrees of freedom\nAIC: 65025\n\nNumber of Fisher Scoring iterations: 4\n\n\nShow the code\n# Trend in the original scale of the outcome cases (deaths) with 95% confidence intervals. \nIRR_trend_pop &lt;- exp(cbind(Estimate=mort_poissontrend_pop$coef, confint(mort_poissontrend_pop)))\n\n(IRR_trend_pop['index', ] - 1) *100\n\n\n  Estimate      2.5 %     97.5 % \n0.01863368 0.01798691 0.01928046 \n\n\nThe summary() function will only provide the coefficients in the original log scale of the poisson model. Another way of obtaining the IRR is by using tbl_regression with the argument exponentiate = TRUE:\n{r: task-4-4-regression-pois-pop-IRR} mort_poissontrend_pop %&gt;%    gtsummary::tbl_regression(     exponentiate = TRUE,     estimate_fun = ~style_number(.x, digits = 4)   )\nplot the predicted and the original values:\n\n\nShow the code\nggplot(data = mortz, mapping = aes(x = date_index)) +\n  geom_point(mapping = aes(y = cases), alpha = 0.5) +\n  geom_line(mapping = aes(y = fitted(mort_poissontrend_pop)), colour = \"blue\", lwd=1.1) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(\n    x = \"Week\", y = \"Mortality rate\",\n    title = \"Spain: Mortality rate per week with fitted trend\"\n  ) +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nCompare the trend and IRR in the models with and without population.\nFor communication, it might be better to present the results in terms of number of deaths per 100.000 inhabitants (population), which would then be called the mortality rate. We will now plot the same trend but as rate per 100.000 inhabitants. For this purpose we must adjust both the observed and the fitted values by multiplying with 100.000.\n\n\nShow the code\nmortz&lt;- mortz %&gt;%\n  mutate(\n    mortality_rate = cases/pop*100000\n  )\n\n\nggplot(data = mortz, mapping = aes(x = date_index)) +\n  geom_point(mapping = aes(y = mortality_rate), alpha = 0.5) +\n  geom_line(mapping = aes(y = fitted(mort_poissontrend_pop)/pop*100000), colour = \"blue\", lwd=1.1) +\n  scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n  labs(\n    x = \"Week\", y = \"Mortality rate (per 100.000 pop)\",\n    title = \"Spain: Mortality rate per 100.000 population per week with fitted trend\"\n  ) +\n  tsa_theme\n\n\n\n\n\n\n\n\n\nHow do you interpret the IRR in the model for mortality rate?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/04-practical.html#task-4-5",
    "href": "scripts/04-practical.html#task-4-5",
    "title": "Practical Session 4: Smoothing and Trends",
    "section": "Task 4.5 (Optional)",
    "text": "Task 4.5 (Optional)\nCarry out the same analysis for men and women. Do you see differences between the two? where?\n\n\nShow the code\n# Save the mortagg and mortz objects.\n#save(mortagg, mortz, file = here(\"data\", \"mortagg_case_4.RData\"))",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 4: Smoothing and Trends"
    ]
  },
  {
    "objectID": "scripts/05-practical.html",
    "href": "scripts/05-practical.html",
    "title": "Practical Session 5: Periodicity",
    "section": "",
    "text": "Periodicity\nShow the code\n# Install pacman if not installed already, and activate it\nrm(list=ls())\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n# Install, update and activate libraries\npacman::p_load(\n  here, \n  rio, \n  skimr,\n  tsibble,\n  TSA,\n  tidyverse,\n  season,\n  lmtest\n)\n\n# Create tsa_theme from previous exercise to be used in ggplot.\ntsa_theme &lt;- theme_bw() + \n        theme(\n            plot.title = element_text(face = \"bold\", \n                                      size = 12),\n            legend.background = element_rect(fill = \"white\", \n                                             size = 4, \n                                             colour = \"white\"),\n            # legend.justification = c(0, 1),\n            legend.position = \"bottom\",\n            panel.grid.minor = element_blank()\n        )",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#learn-5",
    "href": "scripts/05-practical.html#learn-5",
    "title": "Practical Session 5: Periodicity",
    "section": "Expected learning outcomes",
    "text": "Expected learning outcomes\nBy the end of this session, participants should be able to:\n\nassess the existence of periodicity in surveillance data\nfit and interpret models containing a trend and one or several sine and cosine curves on surveillance data to model both trend and periodicity",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#task-5-1",
    "href": "scripts/05-practical.html#task-5-1",
    "title": "Practical Session 5: Periodicity",
    "section": "Task 5.1",
    "text": "Task 5.1\nUsing mortagg2_case_4.RData (created in Practical 4), visually assess the existence of any periodicity in the total number of deaths (cyclical patterns). What do you think the period is, if any? Discuss your results with your peers.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#task-5-2",
    "href": "scripts/05-practical.html#task-5-2",
    "title": "Practical Session 5: Periodicity",
    "section": "Task 5.2",
    "text": "Task 5.2\nUsing mortagg, check statistically for the existence of trend and periodicity. Then, fit a model on the data with a trend and then the relevant period(s).\nHow many different periods are you including in your model? Test statistically if the inclusion of more than one period contributes to the fit of the model. Discuss your results.\nDuring the year, when is mortality highest? When is it lowest? What do you think might be the reason and how would you test for that statistically?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#task-5-4",
    "href": "scripts/05-practical.html#task-5-4",
    "title": "Practical Session 5: Periodicity",
    "section": "Task 5.4 (Optional)",
    "text": "Task 5.4 (Optional)\nAssess the existence of periodicity in the dis1 dataset.",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#solution-5-1",
    "href": "scripts/05-practical.html#solution-5-1",
    "title": "Practical Session 5: Periodicity",
    "section": "Help for Task 5.1",
    "text": "Help for Task 5.1\nRequired source code.\n\n\nShow the code\n# Read data\nload(here(\"data\", \"mortagg2_case_4.RData\"))\n\n\nStep 1)\nExplore the time series. The function decompose() decomposes the time series by applying loess to the series; loess is a smoothing method (https://en.wikipedia.org/wiki/Local_regression).\n\n\nShow the code\n# format the data to a time series object using ts().\nmortz.ts  &lt;- ts(mortz$cases, frequency=52, start=c(2010,1))\n\nplot(decompose(mortz.ts))\n\n\n\n\n\n\n\n\n\nDiscuss the panels obtained. What features seem to be relevant?\nStep 2)\nThis step has already been done in case study 4. Run the code in order to have at hand the results and be able to compare later with other models.\nWe start by inspecting if there is a trend in the data.\n\n\nShow the code\nmort_trend &lt;- glm(cases ~ index, family = \"poisson\", data = mortz)\n\nsummary(mort_trend)\n\n\n\nCall:\nglm(formula = cases ~ index, family = \"poisson\", data = mortz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 8.910e+00  1.007e-03 8850.25   &lt;2e-16 ***\nindex       1.895e-04  3.302e-06   57.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 59215  on 519  degrees of freedom\nAIC: 64841\n\nNumber of Fisher Scoring iterations: 4\n\n\nShow the code\n# incidence rate ratio per week:\nIRR_week &lt;- exp(coef(mort_trend))\nIRR_week\n\n\n(Intercept)       index \n7408.911392    1.000189 \n\n\n\n\nShow the code\nggplot(data = mortz, aes(x = date_index)) +\n    geom_point(aes(y = cases), alpha = 0.4) +\n    geom_line(\n        mapping = aes(y = fitted(mort_trend)),\n        colour = \"green\",\n        lwd=1.5\n    ) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n    scale_y_continuous(limits = c(0, NA)) +\n    labs(x = \"Index\", \n         y = \"Number of deaths\", \n         title = \"Regression model: trend\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nInterpret the weekly incidence rate ratio (IRR).\nStep 3)\nAs there seems to be some periodicity in the first exploration, we proceed to inspect if there is a seasonality component in the time series. Seasonality, periodicity both refer to reoccurring patterns in the data.\nR has a number of functions which produce a periodogram - we will use the periodogram function from the TSA package.\n\n\nShow the code\nmort_period &lt;- TSA::periodogram(mortz$cases)\n\n\n\n\n\n\n\n\n\nThe periodogram function plots the estimated periodogram for a given time series and also returns various useful outputs as a list which we can use for other analyses.\nThis type of plot is interpreted by identifying peaks. Convert the frequencies at which peaks occur to periods by taking the reciprocal.\n\n\nShow the code\nmort_period_recip &lt;-\n    tibble(\n        freq = mort_period$freq,\n        spec = mort_period$spec\n    ) %&gt;%\n    arrange(desc(spec)) %&gt;% \n    mutate(reciprocal_freq = 1 / freq)  # here in weeks\n\nview(mort_period_recip)\n\n\nThe above lines of code extract two variables (freq and spec) from the mort_period_recip object and place them in a tibble data.frame. Both columns are ordered in descending order of the spectral variable. At the top of the table we find the strongest spectral signal, as well as at what periodicity does it appear (see reciprocal frequency). Observe that the reciprocal frequency is in the same units as the data, in this case is in\nweeks.\n\n\nShow the code\n# breaks every 13, equivalent to every 3 months approximately.\n\nggplot(data = mort_period_recip, aes(x = reciprocal_freq, y = 2*spec)) +\n    geom_line() +\n    scale_x_continuous(limits = c(0, 160), breaks=seq(0, 160, 13)) +\n    labs(x = \"Period (weeks)\", \n         y = \"Spectral density\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nWhat is the main periodicity in the mortality data? (use the plot and the table of mort_period_recip).\nIs there another periodicity that is worth checking out?",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  },
  {
    "objectID": "scripts/05-practical.html#solution-5-2",
    "href": "scripts/05-practical.html#solution-5-2",
    "title": "Practical Session 5: Periodicity",
    "section": "Help for Task 5.2",
    "text": "Help for Task 5.2\nAs the periodogram shows periodicity close to 52 weeks, we will use a sine curve of a 52-week period. Note that periodicity with a period of one year is also referred to as seasonality.\nFit a poisson regression of cases with only sine and cosine predictor terms. In order to have an appropriate phase in your model (you do not have to worry about identifying it; this will happen automatically), you need to use both a sine and a cosine curve with the same period. The sum of these two curves gives the periodicity for the specified period and the phase best describing our data.\nThe function cosinor fits a model with sinus and cosinus. You need to specify the unit of time as well as the number of cycles per year. Here the data is weekly, and we assume that there is one cycle per year since we assume a 52-week period.\n\n\nShow the code\n# cosinor() only works with data.frames, not tibble.\nmortz.df &lt;- as.data.frame(mortz)\n\nmort_sincos &lt;-  cosinor(cases ~ 1, date = \"week\", \n                            data = mortz.df, type = \"weekly\", cycles=1,\n                            family = poisson())\n\nsummary(mort_sincos)\n\n\n              Length Class  Mode     \ncall           13    -none- call     \nglm            30    glm    list     \nfitted.plus   521    -none- numeric  \nfitted.values 521    -none- numeric  \nresiduals     521    -none- numeric  \ndate            1    -none- character\n\n\nShow the code\nsummary(mort_sincos$glm)\n\n\n\nCall:\nglm(formula = f, family = family, data = data, offset = offset)\n\nCoefficients:\n             Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept) 8.9557059  0.0004987 17956.87   &lt;2e-16 ***\ncosw        0.1208466  0.0007047   171.50   &lt;2e-16 ***\nsinw        0.0668199  0.0007027    95.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 23972  on 518  degrees of freedom\nAIC: 29600\n\nNumber of Fisher Scoring iterations: 3\n\n\nShow the code\n# Sinus and cosinus curves\nggplot(data=mort_sincos$glm$data, aes(x=week, y=sinw)) +\n    geom_line() +\n    geom_line(aes(x=week, y=cosw)) +\n    scale_x_continuous(limits = c(1, 53)) +\n    labs(x = \"Period (weeks)\", \n         y = \"sincos\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data = mortz, aes(x = date_index)) +\n    geom_point(aes(y = cases), alpha = 0.4) +\n    geom_line(\n        mapping = aes(y = fitted(mort_sincos)),\n        colour = \"green\",\n        lwd=1.5\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n    labs(x = \"Index\", \n         y = \"Number of deaths\", \n         title = \"Regression model: sin, cos terms\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nShow the code\n# Plot residuals\nplot(mort_sincos$res)\n\n\n\n\n\n\n\n\n\nWhen you inspect the residuals of the model with only sinus and cosinus terms, what do you observe?\nIn the next model include a trend and the seasonality. We will calculate the sinus and cosinus by hand.\nAdd a line with the fitted values of the model with only sine and cosine terms.\n\n\nShow the code\n# calculate the sine and cosine terms.\n\nmortz &lt;- mortz %&gt;%\n  mutate(cos52 = cos(2 * pi * index / 52),\n         sin52 = sin(2 * pi * index / 52))\n\nmort_trendsincos &lt;- glm(cases ~ index + sin52 + cos52, family = \"poisson\", data = mortz)\n\nsummary(mort_trendsincos)\n\n\n\nCall:\nglm(formula = cases ~ index + sin52 + cos52, family = \"poisson\", \n    data = mortz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 8.898e+00  1.012e-03 8792.82   &lt;2e-16 ***\nindex       2.184e-04  3.311e-06   65.97   &lt;2e-16 ***\nsin52       9.137e-02  7.066e-04  129.29   &lt;2e-16 ***\ncos52       1.061e-01  7.033e-04  150.88   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 19668  on 517  degrees of freedom\nAIC: 25298\n\nNumber of Fisher Scoring iterations: 3\n\n\nShow the code\nggplot(data = mortz, aes(x = date_index)) +\n    geom_point(aes(y = cases), alpha = 0.4) +\n    geom_line(\n        mapping = aes(y = fitted(mort_trendsincos)),\n        colour = \"green\",\n        lwd=1.5\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n    labs(x = \"Index\", \n         y = \"Number of deaths\", \n         title = \"Regression model: trend, sin, cos terms\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nShow the code\n# Plot residuals\nplot(mort_trendsincos$residual)\n\n\n\n\n\n\n\n\n\nHow does the fit look visually? Which models seems to fit better?\nWhat do you observe in the plot of residuals?\nTo fit the model better, we could try to add more sine/cosine curves, of a period corresponding to the second strongest peak in the model (26 weeks). This will allow for cycles (periods) of not only 52, but also 26 weeks, should we think there might be half-yearly cycles which are relevant. In this case, for instance, there may be elevated mortality in winter, but also in summer during heatwaves.\nGenerate sine and cosine terms with a period of 26 weeks and name them sin26 and cos26. Add these two new terms to the previous model.\nPlot the results, compare with the previous model and comment on it.\n\n\nShow the code\nmortz &lt;-\n    mortz %&gt;%\n    mutate(\n        sin26 = sin(2 * pi * index / 26),\n        cos26 = cos(2 * pi * index / 26)\n    )\n\nmort_trendsin2cos2 &lt;- glm(cases ~ index + sin52 + cos52 + sin26 + cos26,\n                           family = \"poisson\", \n                           data = mortz)\n                           \nsummary(mort_trendsin2cos2)\n\n\n\nCall:\nglm(formula = cases ~ index + sin52 + cos52 + sin26 + cos26, \n    family = \"poisson\", data = mortz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 8.895e+00  1.013e-03 8779.51   &lt;2e-16 ***\nindex       2.265e-04  3.313e-06   68.35   &lt;2e-16 ***\nsin52       9.034e-02  7.142e-04  126.48   &lt;2e-16 ***\ncos52       1.021e-01  6.994e-04  145.94   &lt;2e-16 ***\nsin26       5.075e-02  7.054e-04   71.94   &lt;2e-16 ***\ncos26       3.298e-02  7.034e-04   46.88   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 62509  on 520  degrees of freedom\nResidual deviance: 12288  on 515  degrees of freedom\nAIC: 17922\n\nNumber of Fisher Scoring iterations: 3\n\n\n\n\nShow the code\nggplot(data = mortz, aes(x = date_index)) +\n    geom_point(aes(y = cases), alpha = 0.4) +\n    geom_line(\n        mapping = aes(y = fitted(mort_trendsin2cos2)),\n        colour = \"green\",\n        lwd=1.5\n    ) +\n    scale_y_continuous(limits = c(0, NA)) +\n  scale_x_yearweek(date_labels = \"%Y-%W\", \n                   date_breaks = \"1 year\") +\n    labs(x = \"Index\", \n         y = \"Number of deaths\", \n         title = \"Regression model: trend, sin52, cos52, sin26, cos26 terms\") +\n    tsa_theme\n\n\n\n\n\n\n\n\n\nShow the code\n# Plot residuals\nplot(mort_trendsin2cos2$res)\n\n\n\n\n\n\n\n\n\nIs the addition of variables sin26 and cos26 a statistically significant contribution to your model?\n\n\nShow the code\n# likelihood ratio test: compares two nested models\n\n# test sin26 and cos26\nlrtest(mort_trendsincos, mort_trendsin2cos2)\n\n\nLikelihood ratio test\n\nModel 1: cases ~ index + sin52 + cos52\nModel 2: cases ~ index + sin52 + cos52 + sin26 + cos26\n  #Df   LogLik Df Chisq Pr(&gt;Chisq)    \n1   4 -12644.9                        \n2   6  -8954.9  2  7380  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio test compares two nested models, that is, models with the same variables and data, where one model has additional variables. Here,\nthe only difference between mort_trendsincos and mort_trendsin2cos2 are the sine and cosine at 26 weeks. If the test results in a significant p-value, then the additional variables improve the model significantly, and there is a better fit with the additional variables.\n\n\nShow the code\nsave(list = ls(pattern = 'mort'), file = here(\"data\", \"mortagg2_case_5.RData\"))\n\n#load(here(\"data\",\"mortagg2_case_5.RData\"))",
    "crumbs": [
      "PRACTICALS",
      "Practical Session 5: Periodicity"
    ]
  }
]